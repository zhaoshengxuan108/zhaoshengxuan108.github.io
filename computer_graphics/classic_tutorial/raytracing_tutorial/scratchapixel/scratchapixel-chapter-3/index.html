<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.122.0">
    <meta name="generator" content="Relearn 5.23.2+tip">
    <meta name="description" content="啊啊啊啊">
    <meta name="author" content="Sören Weber">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content="https://example.com/images/hero.png">
    <meta name="twitter:title" content="Rendering an Image of a 3D Scene: an Overview :: Hugo Relearn Theme">
    <meta name="twitter:description" content="It All Starts with a Computer and a Computer Screen Introduction The lesson Introduction to Raytracing: A Simple Method for Creating 3D Images provided you with a quick introduction to some important concepts in rendering and computer graphics in general, as well as the source code of a small ray tracer (with which we rendered a scene containing a few spheres). Ray tracing is a very popular technique for rendering a 3D scene (mostly because it is easy to implement and also a more natural way of thinking of the way light propagates in space, as quickly explained in lesson 1), however other methods exist.">
    <meta property="og:title" content="Rendering an Image of a 3D Scene: an Overview :: Hugo Relearn Theme">
    <meta property="og:description" content="It All Starts with a Computer and a Computer Screen Introduction The lesson Introduction to Raytracing: A Simple Method for Creating 3D Images provided you with a quick introduction to some important concepts in rendering and computer graphics in general, as well as the source code of a small ray tracer (with which we rendered a scene containing a few spheres). Ray tracing is a very popular technique for rendering a 3D scene (mostly because it is easy to implement and also a more natural way of thinking of the way light propagates in space, as quickly explained in lesson 1), however other methods exist.">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://example.com/computer_graphics/classic_tutorial/raytracing_tutorial/scratchapixel/scratchapixel-chapter-3/index.html">
    <meta property="og:image" content="https://example.com/images/hero.png">
    <meta property="article:section" content="Scratchapixe 系列短文 :: Hugo Relearn Theme">
    <meta property="og:site_name" content="Hugo Relearn Theme">
    <title>Rendering an Image of a 3D Scene: an Overview :: Hugo Relearn Theme</title>
    <link href="../../../../../computer_graphics/classic_tutorial/raytracing_tutorial/scratchapixel/scratchapixel-chapter-3/index.xml" rel="alternate" type="application/rss+xml" title="Rendering an Image of a 3D Scene: an Overview :: Hugo Relearn Theme">
    <link href="../../../../../computer_graphics/classic_tutorial/raytracing_tutorial/scratchapixel/scratchapixel-chapter-3/index.print.html" rel="alternate" type="text/html" title="Rendering an Image of a 3D Scene: an Overview :: Hugo Relearn Theme"><link rel="icon" href="../../../../../images/favicon.ico">
    <!-- https://github.com/filamentgroup/loadCSS/blob/master/README.md#how-to-use -->
    <link href="../../../../../css/fontawesome-all.min.css?1708235000" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="../../../../../css/fontawesome-all.min.css?1708235000" rel="stylesheet"></noscript>
    <link href="../../../../../css/nucleus.css?1708235000" rel="stylesheet">
    <link href="../../../../../css/auto-complete.css?1708235000" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="../../../../../css/auto-complete.css?1708235000" rel="stylesheet"></noscript>
    <link href="../../../../../css/perfect-scrollbar.min.css?1708235000" rel="stylesheet">
    <link href="../../../../../css/fonts.css?1708235000" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="../../../../../css/fonts.css?1708235000" rel="stylesheet"></noscript>
    <link href="../../../../../css/theme.css?1708235000" rel="stylesheet">
    <link href="../../../../../css/theme-auto.css?1708235000" rel="stylesheet" id="R-variant-style">
    <link href="../../../../../css/variant.css?1708235000" rel="stylesheet">
    <link href="../../../../../css/print.css?1708235000" rel="stylesheet" media="print">
    <link href="../../../../../css/ie.css?1708235000" rel="stylesheet">
    <script src="../../../../../js/url.js?1708235000"></script>
    <script src="../../../../../js/variant.js?1708235000"></script>
    <script>
      // hack to let hugo tell us how to get to the root when using relativeURLs, it needs to be called *url= for it to do its magic:
      // https://github.com/gohugoio/hugo/blob/145b3fcce35fbac25c7033c91c1b7ae6d1179da8/transform/urlreplacers/absurlreplacer.go#L72
      window.index_js_url="../../../../../index.search.js";
      var root_url="../../../../../";
      var baseUri=root_url.replace(/\/$/, '');
      window.relearn = window.relearn || {};
      window.relearn.baseUriFull='https:\/\/example.com/';
      // variant stuff
      window.variants && variants.init( [ 'auto', 'relearn-light', 'relearn-dark', 'zen-light', 'zen-dark', 'neon', 'learn', 'blue', 'green', 'red' ] );
      // translations
      window.T_Copy_to_clipboard = `Copy to clipboard`;
      window.T_Copied_to_clipboard = `Copied to clipboard!`;
      window.T_Copy_link_to_clipboard = `Copy link to clipboard`;
      window.T_Link_copied_to_clipboard = `Copied link to clipboard!`;
      window.T_Reset_view = `Reset view`;
      window.T_View_reset = `View reset!`;
      window.T_No_results_found = `No results found for "{0}"`;
      window.T_N_results_found = `{1} results found for "{0}"`;
    </script>
    <style>
      #R-body img.bg-white {
        background-color: white;
      }
    </style>
  </head>
  <body class="mobile-support html" data-url="../../../../../computer_graphics/classic_tutorial/raytracing_tutorial/scratchapixel/scratchapixel-chapter-3/index.html">
    <div id="R-body" class="default-animation">
      <div id="R-body-overlay"></div>
      <nav id="R-topbar">
        <div class="topbar-wrapper">
          <div class="topbar-sidebar-divider"></div>
          <div class="topbar-area topbar-area-start" data-area="start">
            <div class="topbar-button topbar-button-sidebar" data-content-empty="disable" data-width-s="show" data-width-m="hide" data-width-l="hide">
              <button class="topbar-control" onclick="toggleNav()" type="button" title="Menu (CTRL&#43;ALT&#43;n)">
                <i class="fa-fw fas fa-bars"></i>
              </button>
            </div>
            <div class="topbar-button topbar-button-toc" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show">
              <button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="Table of Contents (CTRL&#43;ALT&#43;t)">
                <i class="fa-fw fas fa-list-alt"></i>
              </button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
<nav class="TableOfContents">
  <ul>
    <li><a href="#it-all-starts-with-a-computer-and-a-computer-screen">It All Starts with a Computer and a Computer Screen</a>
      <ul>
        <li><a href="#introduction">Introduction</a></li>
        <li><a href="#it-all-start-with-a-computer-and-a-computer-screen">It All Start with a Computer (and a Computer Screen)</a></li>
      </ul>
    </li>
    <li><a href="#and-it-follows-with-a-3d-scene">And It Follows With a 3D Scene</a>
      <ul>
        <li><a href="#triangle-as-the-rendering-primitive">Triangle as the Rendering Primitive</a></li>
        <li><a href="#a-3d-scene-is-more-than-just-geometry">A 3D Scene Is More Than Just Geometry</a></li>
        <li><a href="#summary">Summary</a></li>
      </ul>
    </li>
    <li><a href="#an-overview-of-the-rendering-process-visibility-and-shading">An Overview of the Rendering Process: Visibility and Shading</a>
      <ul>
        <li><a href="#perspective-projection-and-the-visibility-problem">Perspective Projection and the Visibility Problem</a></li>
        <li><a href="#shading">Shading</a></li>
        <li><a href="#light-transport">Light Transport</a></li>
        <li><a href="#summary-1">Summary</a></li>
      </ul>
    </li>
    <li><a href="#perspective-projection">Perspective Projection</a>
      <ul>
        <li><a href="#going-from-3d-to-2d-the-projection-matrix">Going from 3D to 2D: the Projection Matrix</a></li>
      </ul>
    </li>
    <li><a href="#the-visibility-problem">The Visibility Problem</a>
      <ul>
        <li><a href="#rasterisation-to-solve-the-visibility-problem-how-does-it-work">Rasterisation to Solve the Visibility Problem: How Does it Work?</a></li>
        <li><a href="#ray-tracing-to-solve-the-visibility-problem-how-does-it-work">Ray Tracing to Solve the Visibility Problem: How Does It Work?</a></li>
        <li><a href="#comparing-rasterization-and-ray-tracing">Comparing rasterization and ray-tracing</a></li>
        <li><a href="#summary-2">Summary</a></li>
      </ul>
    </li>
    <li><a href="#a-light-simulator">A Light Simulator</a>
      <ul>
        <li><a href="#reflection">Reflection</a></li>
        <li><a href="#transparency">Transparency</a></li>
        <li><a href="#glossy-or-specular-reflection">Glossy or Specular Reflection</a></li>
        <li><a href="#diffuse-reflection">Diffuse Reflection</a></li>
        <li><a href="#subsurface-scattering">Subsurface Scattering</a></li>
        <li><a href="#indirect-diffuse">Indirect Diffuse</a></li>
        <li><a href="#indirect-specular-or-caustics">Indirect Specular or Caustics</a></li>
        <li><a href="#soft-shadows">Soft Shadows</a></li>
        <li><a href="#light-transport-and-shading-two-related-but-different-problems">Light Transport and Shading: Two Related But Different Problems</a></li>
        <li><a href="#global-illumination">Global illumination</a></li>
        <li><a href="#why-is-ray-tracing-better-than-rasterization-is-it-better">Why is ray-tracing better than rasterization? Is it better?</a></li>
      </ul>
    </li>
    <li><a href="#light-transport-1">Light Transport</a>
      <ul>
        <li><a href="#light-transport-2">Light Transport</a></li>
        <li><a href="#summary-3">Summary</a></li>
      </ul>
    </li>
    <li><a href="#shading-1">Shading</a>
      <ul>
        <li><a href="#conclusion">Conclusion</a></li>
      </ul>
    </li>
    <li><a href="#summary-and-other-considerations-about-rendering">Summary and Other Considerations About Rendering</a>
      <ul>
        <li><a href="#summary-4">Summary</a></li>
      </ul>
    </li>
  </ul>
</nav>
                </div>
              </div>
            </div>
          </div>
          <ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype="http://schema.org/BreadcrumbList"><li
            itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement"><a itemprop="item" href="../../../../../index.html"><span itemprop="name">主页</span></a><meta itemprop="position" content="1">&nbsp;>&nbsp;</li><li
            itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement"><a itemprop="item" href="../../../../../computer_graphics/index.html"><span itemprop="name">计算机图形学</span></a><meta itemprop="position" content="2">&nbsp;>&nbsp;</li><li
            itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement"><a itemprop="item" href="../../../../../computer_graphics/classic_tutorial/index.html"><span itemprop="name">经典教程</span></a><meta itemprop="position" content="3">&nbsp;>&nbsp;</li><li
            itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement"><a itemprop="item" href="../../../../../computer_graphics/classic_tutorial/raytracing_tutorial/index.html"><span itemprop="name">光线追踪</span></a><meta itemprop="position" content="4">&nbsp;>&nbsp;</li><li
            itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement"><a itemprop="item" href="../../../../../computer_graphics/classic_tutorial/raytracing_tutorial/scratchapixel/index.html"><span itemprop="name">Scratchapixe 系列短文</span></a><meta itemprop="position" content="5">&nbsp;>&nbsp;</li><li
            itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement"><span itemprop="name">Rendering an Image of a 3D Scene: an Overview</span><meta itemprop="position" content="6"></li>
          </ol>
          <div class="topbar-area topbar-area-end" data-area="end">
            <div class="topbar-button topbar-button-print" data-content-empty="disable" data-width-s="area-more" data-width-m="show" data-width-l="show">
              <a class="topbar-control" href="../../../../../computer_graphics/classic_tutorial/raytracing_tutorial/scratchapixel/scratchapixel-chapter-3/index.print.html" title="Print whole chapter (CTRL&#43;ALT&#43;p)">
                <i class="fa-fw fas fa-print"></i>
              </a>
            </div>
            <div class="topbar-button topbar-button-prev" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show">
              <a class="topbar-control" href="../../../../../computer_graphics/classic_tutorial/raytracing_tutorial/scratchapixel/scratchapixel-chapter-2/index.html" title="Where Do I Start? A Very Gentle Introduction to Computer Graphics Programming (🡐)">
                <i class="fa-fw fas fa-chevron-left"></i>
              </a>
            </div>
            <div class="topbar-button topbar-button-next" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show">
              <a class="topbar-control" href="../../../../../computer_graphics/classic_tutorial/raytracing_tutorial/scratchapixel/scratchapixel-chapter-4/index.html" title="Computing the Pixel Coordinates of a 3D Point (🡒)">
                <i class="fa-fw fas fa-chevron-right"></i>
              </a>
            </div>
            <div class="topbar-button topbar-button-more" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show">
              <button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="More">
                <i class="fa-fw fas fa-ellipsis-v"></i>
              </button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
                  <div class="topbar-area topbar-area-more" data-area="more">
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </nav>
      <div id="R-main-overlay"></div>
      <main id="R-body-inner" class="highlightable default" tabindex="-1">
        <div class="flex-block-wrapper">
          <article class="default">
            <header class="headline">
            </header>
<h1 id="rendering-an-image-of-a-3d-scene-an-overview">Rendering an Image of a 3D Scene: an Overview</h1>

<h2 id="it-all-starts-with-a-computer-and-a-computer-screen">It All Starts with a Computer and a Computer Screen</h2>
<h3 id="introduction">Introduction</h3>
<p>The lesson <a href="#introduction-to-raytracing-a-simple-method-for-creating-3d-images">Introduction to Raytracing: A Simple Method for Creating 3D Images</a> provided you with a quick introduction to some important concepts in rendering and computer graphics in general, as well as the source code of a small ray tracer (with which we rendered a scene containing a few spheres). Ray tracing is a very popular technique for rendering a 3D scene (mostly because it is easy to implement and also a more natural way of thinking of the way light propagates in space, as quickly explained in lesson 1), however other methods exist. In this lesson, we will look at what rendering means, what sort of problems we need to solve to render an image of a 3D scene as well as quickly review the most important techniques that were developed to solve these problems specifically; our studies will be focused on the <strong>ray tracing</strong> and <strong>rasterization</strong> method, two popular algorithms used to solve the visibility problem (finding out which objects making up the scene is visible through the camera). We will also look at shading, the step in which the appearance of the objects as well as their brightness is defined.</p>
<h3 id="it-all-start-with-a-computer-and-a-computer-screen">It All Start with a Computer (and a Computer Screen)</h3>
<p><a href="#R-image-12f12423c722c31cc98d784d1fd26030" class="lightbox-link"><img src="../assets/hello.jpeg" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-12f12423c722c31cc98d784d1fd26030"><img src="../assets/hello.jpeg" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p>The journey in the world of computer graphics starts&hellip; with a computer. It might sound strange to start this lesson by stating what may seem obvious to you, but it is so obvious that we do take this for granted and never think of what it means when it comes to making images with a computer. More than a computer, what we should be concerned about is how we display images with a computer: the computer screen. Both the computer and the computer screen have something important in common. They work with <strong>discrete structures</strong> to the contrary of the world around us, which is made of continuous structures (at least at the macroscopic level). These discrete structures are the <strong>bit</strong> for the computer and the <strong>pixel</strong> for the screen. Let&rsquo;s take a simple example. Take a thread in the real world. It is indivisible. But the representation of this thread onto the surface of a computer screen requires to &ldquo;cut&rdquo; or &ldquo;break&rdquo; it down into small pieces called <strong>pixels</strong>. This idea is illustrated in figure 1.</p>
<p><a href="#R-image-2ce501ad1cb1b4977e1e4179e4a38e99" class="lightbox-link"><img src="../assets/discrete.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-2ce501ad1cb1b4977e1e4179e4a38e99"><img src="../assets/discrete.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 1:</strong> <em>in the real world, everything is &ldquo;continuous&rdquo;. But in the world of computers, an image is made of discrete blocks, the pixels.</em></p>
<p><a href="#R-image-6393bb0cccf51f39ebd03dde236aebb5" class="lightbox-link"><img src="../assets/discrete2.gif" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-6393bb0cccf51f39ebd03dde236aebb5"><img src="../assets/discrete2.gif" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 2:</strong> <em>the process of representing an object on the surface of a computer can be seen as if a grid was laid out on the surface of the object. Every pixel of that grid overlapping the object is filled in with the color of the underlying object. But what happens when the object only partially overlaps the surface of a pixel? Which color should we fill the pixel with?</em></p>
<p>In computing, the process of actually converting any continuous object (a continuous function in mathematics, a digital image of a thread) is called <strong>discretization</strong>. Obvious? Yes and yet, most problems if not all problems in computer graphics come from the very nature of the technology a computer is based on: 0, 1, and pixels.</p>
<p>You may still think &ldquo;who cares?&rdquo;. For someone watching a video on a computer, it&rsquo;s probably not very important indeed. But if you have to create this video, this is probably something you should care about. Think about this. Let&rsquo;s imagine we need to represent a sphere on the surface of a computer screen. Let&rsquo;s look at a sphere and apply a grid on top of it. The grid represents the pixels your screen is made of (figure 2). The sphere overlaps some of the pixels completely. Some of the pixels are also empty. However, some of the pixels have a problem. The sphere overlaps them only partially. In this particular case, what should we fill the pixel with: the color of the background or the color of the object?</p>
<p>Intuitively you might think &ldquo;if the background occupies 35% of the pixel area, and the object 75%, let&rsquo;s assign a color to the pixel which is composed of the background color for 35% and of the object color for 75%&rdquo;. This is pretty good reasoning, but in fact, you just moved the problem around. How do you compute these areas in the first place anyway? One possible solution to this problem is to subdivide the pixel into sub-pixels and count the number of sub-pixels the background overlaps and assume all over sub-pixels are overlapped by the object. The area covered by the background can be computed by taking the number of sub-pixels overlapped by the background over the total number of sub-pixels.</p>
<p><a href="#R-image-a007901c63e1b76e1e2a90e69e3e4b26" class="lightbox-link"><img src="../assets/supersampling.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-a007901c63e1b76e1e2a90e69e3e4b26"><img src="../assets/supersampling.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 4:</strong> <em>to approximate the color of a pixel which is both overlapping a shape and the background, the surface can be subdivided into smaller cells. The pixel&rsquo;s color can be found by computing the number of cells overlapping the shape multiplied by the shape&rsquo;s color plus the number of cells overlapping the background multiplied by the background color, divided by the entire number of cells. However, no matter how small the cells are, some of them will always overlap both the shape and the background.</em></p>
<p>However, no matter how small the sub-pixels are, there will always be some of them overlapping both the background and the object. While you might get a pretty good approximation of the object and background coverage that way (the smaller the sub-pixels the better the approximation), it will always just be an approximation. Computers can only <strong>approximate</strong>. Different techniques can be used to compute this approximation (subdividing the pixel into sub-pixels is just one of them), but what we need to remember from this example, is that a lot of the problems we will have to solve in computer sciences and computer graphics, comes from having to &ldquo;simulate&rdquo; the world which is made of continuous structures with discrete structures. And having to go from one to the other raises all sorts of complex problems (or maybe simple in their comprehension, but complex in their resolution).</p>
<p>Another way of solving this problem is also obviously to increase the resolution of the image. In other words, to represent the same shape (the sphere) using more pixels. However, even then, we are limited by the resolution of the screen.</p>
<p>Images and screens using a two-dimensional array of pixels to represent or display images are called raster graphics and raster displays respectively. The term <strong>raster</strong> more generally defines a grid of x and y coordinates on a display space. We will learn more about rasterization, in the chapter on perspective projection.</p>
<p>As suggested, the main issue with representing images of objects with a computer is that the object shapes need to be &ldquo;broken&rdquo; down into discrete surfaces, the pixels. Computers more generally can only deal with discrete data, but more importantly, the definition with which numbers can be defined in the memory of the computer is limited by the number of bits used to encode these numbers. The number of colors for example that you can display on a screen is limited by the number of bits used to encode RGB values. In the early days of computers, a single bit was used to encode the &ldquo;brightness&rdquo; of pixels on the screen. When the bit had the value 0 the pixel was black and when it was 1, the pixel would be white. The first generation of computers used color displays, encoded color using a single byte or 8 bits. With 8 bits (3 bits for the red channel, 3 bits for the green channel, and 2 bits for the blue channel) you can only define 256 distinct colors (2^3 * 2^3 * 2^2). What happens then when you want to display a color which is not one of the colors you can use? The solution is to find the closest possible matching color from the palette to the color you ideally want to display and display this matching color instead. This process is called color quantization.</p>
<p><a href="#R-image-93018bc1f83b31793a016ce5a394080d" class="lightbox-link"><img src="../assets/posterisation.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-93018bc1f83b31793a016ce5a394080d"><img src="../assets/posterisation.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 5:</strong> <em>our eyes can perceive very small color variations. When too few bits are used to encode colors, banding occurs (right).</em></p>
<p>The problem with color quantization is that when we don&rsquo;t have enough colors to accurately sample a continuous gradation of color tones, continuous gradients appear as a series of discrete steps or bands of color. This effect is called banding (it&rsquo;s also known under the term posterization or false contouring).</p>
<p>There&rsquo;s no need to care about banding so much these days (the most common image formats use 32 bits to encode colors. With 32 bits you can display about 16 million distinct colors), however, keep in mind that fundamentally, colors and pretty much any other continuous function that we may need to represent in the memory of a computer, have to be broken down into a series of discrete or single <strong>quantum</strong> values for which precision is limited by the number of bits used to encode these values.</p>
<p><a href="#R-image-f4c20b13ba70c60898f00e294de1a5af" class="lightbox-link"><img src="../assets/teapot-in-a-pixel.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-f4c20b13ba70c60898f00e294de1a5af"><img src="../assets/teapot-in-a-pixel.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 6:</strong> <em>the shape of objects smaller than a pixel can&rsquo;t be accurately captured by a digital image.</em></p>
<p>Finally, having to break down a continuous function into discrete values may lead to what&rsquo;s known in signal processing and computer graphics as <strong>aliasing</strong>. The main problem with digital images is that the amount of details you can capture depends on the image resolution. The main issue with this is that small details (roughly speaking, details smaller than a pixel) can&rsquo;t be captured by the image accurately. Imagine for example that you want to take a photograph with a digital camera of a teapot that is so far away though that the object is smaller than a pixel in the image (figure 6). A pixel is a discrete structure thus we can only fill it up with a constant color. If in this example, we fill it up with the teapot&rsquo;s color (assuming the teapot has a constant color which is probably not the case if it&rsquo;s shaded), your teapot will only show up as a dot in the image: you failed to capture the teapot&rsquo;s shape (and shading). In reality, aliasing is far more complex than that, but you should know about the term and know for now and should keep in mind that by the very nature of digital images (because pixels are discrete elements), an image of a given resolution can only accurately represent objects of a given size. We will explain what the relationship between the objects size and the image resolution is in the lesson on Aliasing (which you can find in the Mathematics and Physic of Computer Graphics section).</p>
<blockquote>
<details>
 Images are just a collection of pixels. As mentioned before, when an image of the real world is stored in a digital image, shapes are broken down into discrete structures, the pixels. The main drawback of raster images (and raster screens) is that the resolution of the images we > can store or display is limited by the image or the screen resolution (its dimension in pixels).  Zooming in doesn't reveal more details in the image. <strong>Vector graphics</strong> were designed to address this issue. With vector graphics, you do not store pixels but represent the shape of objects (and their colors) using mathematical expressions. That way, rather than being limited by the image resolution, the shapes defined in the file can be rendered on the fly at  the desired resolution, producing an image of the object's shapes that is always perfectly  sharp.
<p><a href="#R-image-aaee75f7e9f42da14e6f8b152218c1e7" class="lightbox-link"><img src="../assets/VectorBitmapExample.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-aaee75f7e9f42da14e6f8b152218c1e7"><img src="../assets/VectorBitmapExample.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
</details>
</blockquote>
<p>To summarize, computers work with quantum values when in fact, processes from the real world that we want to simulate with computers, are generally (if not always) continuous (at least at the macroscopic and even microscopic scale). And in fact, this is a very fundamental issue that is causing all sorts of very puzzling problems, to which a very large chunk of computer graphics research and theory is devoted.</p>
<blockquote>
<details>
Another field of computer graphics in which the discrete representation of the world is a particular issue is fluid simulation. The flow of fluids by their very nature is a continuous process, but to simulate the motion of fluids with a computer, we need to divide space into "discrete" structures generally small cubes called cells.
</details>
</blockquote>
<h2 id="and-it-follows-with-a-3d-scene">And It Follows With a 3D Scene</h2>
<p>Before we can speak about rendering, we need to consider what we are going to render, and what we are looking at. If you have nothing to look at, there is nothing to render.</p>
<p>The real world is made of objects having a very wild variety of shapes, appearances, and structures. For example, what&rsquo;s the difference between smoke, a chair, and water making up the ocean? In computer graphics, we generally like to see objects as either being solid or not. However, in the real world, the only thing that differentiates the two is the density of matter making up these objects. Smoke is made of molecules loosely connected and separated by a large amount of empty space, while wood making up a chair is made of molecules tightly packed into the smallest possible space. In CG though, we generally just need to define the object&rsquo;s external shape (we will speak about how we render non-solid objects later on in this lesson). How do we do that?</p>
<p>In the previous lesson, <a href="../../../../../lessons/3d-basic-rendering/get-started/">Where Do I Start? A Gentle Introduction to Computer Graphics Programming</a>, we already introduced the idea that defining shape within the memory of a computer, we needed to start defining the concept of point in 3D space. Generally, a point is defined as three floats within the memory of a computer, one for each of the three-axis of the Cartesian coordinate system: the x-, y- and z-axis. From here, we can simply define several points in space and connect them to define a surface (a polygon). Note that polygons should always be coplanar which means that all points making up a face or polygon should lie on the same plane. With three points, you can create the simplest possible shape of all: a <strong>triangle</strong>. You will see triangles used everywhere especially in ray-tracing because many different techniques have been developed to efficiently compute the intersection of a line with a triangle. When faces or polygons have more than three points (also called vertices), it&rsquo;s not uncommon to convert these faces into triangles, a process called <strong>triangulation</strong>.</p>
<p><a href="#R-image-858972fcfc9156380f2f3fe87161f996" class="lightbox-link"><img src="../assets/triangle.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-858972fcfc9156380f2f3fe87161f996"><img src="../assets/triangle.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 1:</strong> <em>the basic brick of all 3D objects is a triangle. A triangle can be created by connecting (in 2D or 3D) 3 points or vertices to each other. More complex shapes can be created by assembling triangles.</em></p>
<p>We will explain later in this lesson, why converting geometry to triangles is a good idea. But the point here is that the simplest possible surface or object you can create is a triangle, and while a triangle on its own is not very useful, you can though create more complex shapes by assembling triangles. In many ways, this is what modeling is about. The process is very similar to putting bricks together to create more complex shapes and surfaces.</p>
<p><a href="#R-image-fb00ac2851e8d138e21dcd855a87354f" class="lightbox-link"><img src="../assets/segment.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-fb00ac2851e8d138e21dcd855a87354f"><img src="../assets/segment.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 2:</strong> <em>to approximate a curved surface we sample the curve along the path of the curve and connect these samples.</em></p>
<p><a href="#R-image-7b051db4d5ffc41d33ff7ab274e14455" class="lightbox-link"><img src="../assets/tesselation.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-7b051db4d5ffc41d33ff7ab274e14455"><img src="../assets/tesselation.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 3:</strong> <em>you can use more triangles to improve the curvature of surfaces, but the geometry will become heavier to render.</em></p>
<p>The world is not polygonal!</p>
<p>Most people new to computer graphics often ask though, how can curved surfaces be created from triangles, when a triangle is a flat and angular surface. First, the way we define the surface of objects in CG (using triangles or polygons) is a very crude representation of reality. What may seem like a flat surface (for example the surface of a wall) to our eyes, is generally an incredibly complex landscape at the microscopic level. Interestingly enough, the microscopic structure of objects has a great influence on their appearance, not on their overall shape. Something worth keeping in mind. But to come back to the main question, using triangles or polygons is indeed not the best way of representing curved surfaces. It gives a faceted look to objects, a little bit like a cut diamond (this facet look can be slightly improved with a technique called <strong>smooth shading</strong>, but smooth shading is just a trick we will learn about when we go to the lessons on shading). If you draw a smooth curve, you can approximate this curve by placing a few points along this curve and connecting these points with straight lines (which we call segments). To improve this approximation you can simply reduce the size of the segment (make them smaller) which is the same as creating more points along the curve. The process of actually placing points or vertices along a smooth surface is called <strong>sampling</strong> (the process of converting a smooth surface to a triangle mesh is called <strong>tessellation</strong>. We will explain further in this chapter how smooth surfaces can be defined). Similarly, with 3D shapes, we can create more and smaller triangles to better approximate curved surfaces. Of course, the more geometry (or triangles) we create, the longer it will take to render this object. This is why the art of rendering is often to find a tradeoff between the amount of geometry you use to approximate the curvature of an object and the time it takes to render this 3D model. The amount of geometric detail you put in a 3D model also depends on how close you will see this model in your image. The closer you are to the object, the more details you may want to see. Dealing with model complexity is also a very large field of research in computer graphics (a lot of research has been done to find automatic/adaptive ways of adjusting the number of triangles an object is made of depending on its distance to the camera or the curvature of the object).</p>
<blockquote>
<details>
In other words, it is impossible to render a perfect circle or a perfect sphere with polygons or triangles. However, keep in mind that computers work on discrete structures, as do monitors. There is no reason for a renderer to be able to perfectly render shapes like circles if they'll just be displayed using a raster screen in the end. The solution (which has been around for decades now) is simply to use triangles that are smaller than a pixel, at which point no one looking at the monitor can tell that your basic primitive is a triangle. This idea has been used very widely in high-quality rendering software such as Pixar's RenderMan, and in the past decade, it has appeared in real-time applications as well (as part of the tessellation process).
</details>
</blockquote>
<p><a href="#R-image-01674c48b4e9a51e9b6c034209d0c087" class="lightbox-link"><img src="../assets/bezierpatch.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-01674c48b4e9a51e9b6c034209d0c087"><img src="../assets/bezierpatch.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 4:</strong> <em>a Bezier patch and its control points which are represented in this image by the orange net. Note how the resulting surface is not passing through the control points or vertices (excepted at the edge of the surface which is a property of Bezier patches actually).</em></p>
<p><a href="#R-image-25b9813e59d88fa5ea2349863d239513" class="lightbox-link"><img src="../assets/subdivision.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-25b9813e59d88fa5ea2349863d239513"><img src="../assets/subdivision.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 5:</strong> <em>a cube turned into a sphere (almost a sphere) using the subdivision surface algorithm. The idea behind this algorithm is to create a smoother version of the original mesh by recursively subdividing it.</em></p>
<p>Polygonal meshes are easy which is why they are popular (most objects you see in CG feature films or video games are defined that way: as an assembly of polygons or triangles) however as suggested before, they are not great to model curved or organic surfaces. This became a particular issue when computers started to be used to design manufactured objects such as cars (CAD). NURBS or Subdivision surfaces were designed to address this particular shortcoming. They are based on the idea that points only define a control mesh from which a perfect curved surface can be computed mathematically. The surface itself is purely the result of an equation thus it can not be rendered directly (nor is the control mesh which is only used as an input to the algorithm). It needs to be sampled, similarly to the way we sampled the curve earlier on (the points forming the base or input mesh are usually called control points. One of the characteristics of these techniques is that the resulting surface, in general, does not pass through these control points). The main advantage of this approach is that you need fewer points (fewer compared to the number of points required to get a smooth surface with polygons) to control the shape of a perfectly smooth surface, which can then be converted to a triangular mesh smoother than the original input control mesh. While it is possible to create curved surfaces with polygons, editing them is far more time-consuming (and still less accurate) than when similar shapes can be defined with just a few points as with NURBS and Subdivision surfaces. If they are superior, why are they not used everywhere? They almost are. They are (slightly) more expansive to render than polygonal meshes because a polygonal mesh needs to be generated from the control mesh first (it takes an extra step), which is why they are not always used in video games (but many game engines such as the Cry Engine implement them), but they are in films. NURBS are slightly more difficult to manipulate overall than polygonal meshes. This is why artists generally use subdivision surfaces instead, but they are still used in design and CAD, where a high degree of precision is needed. Nurbs and Subdivisions surfaces will be studied in the Geometry section, however, in a further lesson in this section, we will learn about Bezier curves and surfaces (to render the Utah teapot), which in a way, are quite similar to NURBS.</p>
<blockquote>
<details>
NURBS and Subdivision surfaces are not similar. NURBS are indeed defined by a mathematical equation. They are part of a family of surfaces called parametric surfaces (see below). Subdivision surfaces are more the result of a 'process' applied to the input mesh, to smooth its surface by recursively subdividing it. Both techniques are detailed in the Geometry section.
</details>
</blockquote>
<p><a href="#R-image-7155d8af4e9cf906d0e4959e93da3b0f" class="lightbox-link"><img src="../assets/volume.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-7155d8af4e9cf906d0e4959e93da3b0f"><img src="../assets/volume.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 6:</strong> <em>to represent fluids such as smoke or liquids, we need to store information such as the volume density in the cells of a 3D grid.</em></p>
<p>In most cases, 3D models are generated by hand. By hand, we mean that someone creates vertices in 3D space and connects them to make up the faces of the object. However, it is also possible to use simulation software to generate geometry. This is generally how you create water, smoke, or fire. Special programs simulate the way fluids move and generate a polygon mesh from this simulation. In the case of smoke or fire, the program will not generate a surface but a 3D dimensional grid (a rectangle or a box that is divided into equally spaced cells also called <strong>voxels</strong>). Each cell of this grid can be seen as a small volume of space that is either empty or occupied by smoke. Smoke is mostly defined by its density which is the information we will store in the cell. Density is just a float, but since we deal with a 3D grid, a 512x512x512 grid already consumes about 512Mb of memory (and we may need to store more data than just density such as the smoke or fire temperature, its color, etc.). The size of this grid is 8 times larger each time we double the grid resolution (a 1024x1024x1024 requires 4Gb of storage). Fluid simulation is computationally intensive, the simulation generates very large files, and rendering the volume itself generally takes more time than rendering solid objects (we need to use a special algorithm known as ray-marching which we will briefly introduce in the next chapters). In the image above (figure 6), you can see a screenshot of a 3D grid created in Maya.</p>
<p>When ray tracing is used, it is not always necessary to convert an object into a polygonal representation to render it. Ray tracing requires computing the intersection of rays (which are simply lines) with the geometry making up the scene. Finding if a line (a ray) intersects a geometrical shape, can sometimes be done mathematically. This is either possible because:</p>
<ul>
<li>
<p>a <strong>geometric solution</strong> or,</p>
</li>
<li>
<p>an <strong>algebraic solution</strong> exists to the ray-object intersection test. This is generally possible when the shape of the object can be defined mathematically, with an equation. More generally, you can see this equation, as a function representing a surface (such as the surface of a sphere) overall space. These surfaces are called <strong>implicit surfaces</strong> (or algebraic surfaces) because they are defined implicitly, by a function. The principle is very simple. Imagine you have two equations:</p>

<span class="math align-center">$$
\begin{array}{l}
y = 2x + 2\\
y = -2x.\\
\end{array}
$$</span><p><a href="#R-image-eb6ae0cf19723916a3504d62c148322c" class="lightbox-link"><img src="../assets/equation.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-eb6ae0cf19723916a3504d62c148322c"><img src="../assets/equation.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p>You can see a plot of these two equations in the adjacent image. This is an example of a <a href="http://en.wikipedia.org/wiki/System_of_linear_equations" target="_blank">system of linear equations</a>. If you want to find out if the two lines defined by these equations meet in one point (which you can see as an intersection), then they must have one x for which the two equations give the same y. Which you can write as:</p>

<span class="math align-center">$$
2x + 2 = -2x.
$$</span><p>Solving for x, you get:</p>

<span class="math align-center">$$
\begin{array}{l}
4x + 2 = 0\\
4x = -2\\
x = -\dfrac{1}{2}\\ 
\end{array}
$$</span><p>Because a ray can also be defined with an equation, the two equations, the equation of the ray and the equation defining the shape of the object, can be solved like any other system of linear equations. If a solution to this system of linear equations exists, then the ray intersects the object.</p>
</li>
</ul>
<p>A very good and simple example of a shape whose intersection with a ray can be found using the geometric and algebraic method is a sphere. You can find both methods explained in the lesson <a href="../../../../../lessons/3d-basic-rendering/minimal-ray-tracer-rendering-simple-shapes/">Rendering Simple Shapes</a>.</p>
<blockquote>
<details>
_What is the difference between parametric and implict surfaces_
Earlier on in the lesson, we mentioned that NURBS and Subdivision surfaces were also somehow defined mathematically. While this is true, there is a difference between NURBS and implicit surfaces (Subdivision surface can also be considered as a separate case, in which the base mesh is processed to produce a smoother and higher resolution mesh). NURBS are defined by what we call a parametric equation, an equation that is the function of one or several parameters. In 3D, the general form of this equation can be defined as follow: 

<span class="math align-center">$$
f(u,v) = (x(u,v), y(u,v), z(u,v)).
$$</span><p>The parameters u and v are generally in the range of 0 to 1. An implicit surface is defined by a polynomial which is a function of three variables: x, y, and z.</p>

<span class="math align-center">$$
p(x, y, z) = 0.
$$</span><p>For example, a sphere of radius R centered at the origin is defined parametrically with the following equation:</p>

<span class="math align-center">$$
f(\theta, \phi) = (\sin(\theta)\cos(\phi), \sin(\theta)\sin(\phi), \cos(\theta)).
$$</span><p>Where the parameters u and v are actually being replaced in this example by (\theta) and (\phi) respectively and where (0 \leq \theta \leq \pi) and (0 \leq \phi \leq 2\pi). The same sphere defined implicitly has the following form:</p>

<span class="math align-center">$$
x^2 + y^2 + z^2 - R^2 = 0.
$$</span></details>
</blockquote>
<p><a href="#R-image-3e253703ed48021f6f5317095028a488" class="lightbox-link"><img src="../assets/metaball.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-3e253703ed48021f6f5317095028a488"><img src="../assets/metaball.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 7:</strong> <em>metaballs are useful to model organic shapes.</em></p>
<p><a href="#R-image-7e5b46d96cbb6b9162b908ec1e098548" class="lightbox-link"><img src="../assets/boolean.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-7e5b46d96cbb6b9162b908ec1e098548"><img src="../assets/boolean.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 8:</strong> <em>example of constructive geometry. The volume defined by the sphere was removed from the cube. You can see the two original objects on the left, and the resulting shape on the right.</em></p>
<p>Implicit surfaces are very useful in modeling but are not very common (and certainly less common than they used to be). It is possible to use implicit surfaces to create more complex shapes (implicit primitives such as spheres, cubes, cones, etc. are combined through boolean operations), a technique called <a href="http://en.wikipedia.org/wiki/Constructive_solid_geometry" target="_blank">constructive solid geometry</a> (or CSG). <a href="http://en.wikipedia.org/wiki/Metaballs" target="_blank">Metaballs</a> (invented in the early 1980s by Jim Blinn) is another form of implicit geometry used to create organic shapes.</p>
<p>The problem though with implicit surfaces is that they are not easy to render. While it&rsquo;s often possible to ray trace them directly (we can compute the intersection of a ray with an implicit surface using an algebraic approach, as explained earlier), they first need to be converted to a mesh otherwise. The process of converting an implicit surface to a mesh is not as straightforward as with NURBS or Subdivision surface and requires a special algorithm such as the <a href="http://en.wikipedia.org/wiki/Marching_cubes" target="_blank">marching cube</a> algorithm (proposed by Lorensen and Cline in 1987). It can also potentially lead to creating heavy meshes.</p>
<p>Check the section on Geometry, to read about these different topics in detail.</p>
<h3 id="triangle-as-the-rendering-primitive">Triangle as the Rendering Primitive</h3>
<p>In this series of lessons, we will study an example of an implicit surface with the ray-sphere intersection test. We will also see an example of a parametric surface, with the Utah teapot, which is using Bezier surfaces. However, in general, most rendering APIs choose the solution of actually converting the different geometry types to a triangular mesh and render the triangular mesh instead. This has several advantages. Supporting several geometry types such as polygonal meshes, implicitly or parametric surfaces requires writing a ray-object routine for each supported surface type. This is not only more code to write (with the obvious disadvantages it may have), but it is also difficult if you make this choice, to make these routines work in a general framework, which often results in downgrading the performance of the render engine.</p>
<p>Keep in mind that rendering is more than just rendering 3D objects. It also needs to support many features such as motion blur, displacement, etc. Having to support many different geometry surfaces, means that each one of these surfaces needs to work with the entire set of supported features, which is much harder than if all surfaces are converted to the same rendering primitive, and if we make all the features work for this one single primitive only.</p>
<p>You also generally get better performances if you limit your code to rendering one primitive only because you can focus all your efforts to render this one single primitive very efficiently. Triangles have generally been the preferred choice for ray tracing. A lot of research has been done in finding the best possible (fastest/least instructions, least memory usage, and most stable) algorithm to compute the intersection of a ray with a triangle. However, other rendering APIs such as OpenGL also render triangles and triangles only, even though they don&rsquo;t use the ray tracing algorithm. Modern GPUs in general, are designed and optimized to perform a single type of rendering based on triangles. Someone (humorously) wrote on this topic:</p>
<blockquote>
<p>Because current GPUs are designed to work with triangles, people use triangles and so GPUs only need to process triangles, and so they&rsquo;re designed to process only triangles.</p>
</blockquote>
<p>Limiting yourself to rendering one primitive only, allows you to build common operations directly into the hardware (you can build a component that is extremely good at performing these operations). Generally, triangles are nice to work with for plenty of reasons (including those we already mentioned). They are always coplanar, they are easy to subdivide into smaller triangles yet they are indivisible. The maths to interpolate texture coordinates across a triangle are also simple (something we will be using later to apply a texture to the geometry). This doesn&rsquo;t mean that a GPU could not be designed to render any other kind of primitives efficiently (such as quads).</p>
<blockquote>
<details>
_Can I use quads instead of triangles?_
The triangle is not the only possible primitive used for rendering. The quad can also be used. Modeling or surfacing algorithms such as those that generate subdivision surfaces only work with quads. This is why quads are commonly found in 3D models. Why wasting time triangulating these models if we could render quads as efficiently as triangles? It happens that even in the context of ray-tracing, using quads can sometimes be better than using triangles (in addition to not requiring a triangulation which is a waste when the model is already made out of quads as just suggested). Ray-tracing quads will be addressed in the advanced section on ray-tracing.
</details>
</blockquote>
<h3 id="a-3d-scene-is-more-than-just-geometry">A 3D Scene Is More Than Just Geometry</h3>
<p>Typically though a 3D scene is more than just geometry. While geometry is the most important element of the scene, you also need a camera to look at the scene itself. Thus generally, a scene description also includes a camera. And a scene without any light would be black, thus a scene also needs lights. In rendering, all this information (the description of the geometry, the camera, and the lights) is contained within a file called the scene file. The content of the 3D scene can also be loaded into the memory of a 3D package such as Maya or Blender. In this case, when a user clicks on the render button, a special program or plugin will go through each object contained in the scene, each light, and export the whole lot (including the camera) directly to the renderer. Finally, you will also need to provide the renderer with some extra information such as the resolution of the final image, etc. These are usually called global render settings or options.</p>
<h3 id="summary">Summary</h3>
<p>What you should remember from this chapter is that we first need to consider what a scene is made of before considering the next step, which is to create an image of that 3D scene. A scene needs to contain three things: geometry (one or several 3D objects to look at), lights (without which the scene will be black), and a camera, to define the point of view from which the scene will be rendered. While many different techniques can be used to describe geometry (polygonal meshes, NURBS, subdivision surfaces, implicit surfaces, etc.) and while each one of these types may be rendered directly using the appropriate algorithm, it is easier and more efficient to only support one rendering primitive. In ray tracing and on modern GPUs, the preferred rendering primitive is the triangle. Thus, generally, geometry will be converted to triangular meshes before the scene gets rendered.</p>
<h2 id="an-overview-of-the-rendering-process-visibility-and-shading">An Overview of the Rendering Process: Visibility and Shading</h2>
<p>An image of a 3D scene can be generated in multiple ways, but of course, any way you choose should produce the same image for any given scene. In most cases, the goal of rendering is to create a photo-realistic image (non-photorealistic rendering or NPR is also possible). But what does it mean, and how can this be achieved? Photorealistic means essentially that we need to create an image so &ldquo;real&rdquo; that it looks like a photograph or (if photography didn&rsquo;t exist) that it would look like reality to our eyes (like the reflection of the world off the surface of a mirror). How do we do that? By understanding the laws of physics that make objects appear the way they do, and simulating these laws on the computer. In other words, rendering is nothing else than simulating the laws of physics responsible for making up the world we live in, as it appears to us. Many laws are contributing to making up this world, but fewer contribute to how it looks. For example, gravity, which plays a role in making objects fall (gravity is used in solid-body simulation), has little to do with the way an orange looks like. Thus, in rendering, we will be interested in what makes objects look the way they do, which is essentially the result of the way light propagates through space and interacts with objects (or matter more precisely). This is what we will be simulating.</p>
<h3 id="perspective-projection-and-the-visibility-problem">Perspective Projection and the Visibility Problem</h3>
<p>But first, we must understand and reproduce how objects look to our eyes. Not so much in terms of their appearance but more in terms of their shape and their size with respect to their distance to the eye. The human eye is an optical system that converges light rays (light reflected from an object) to a focus point.</p>
<p><a href="#R-image-0da5388f677e45f20dd3f175dc622e8c" class="lightbox-link"><img src="../assets/perspective1.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-0da5388f677e45f20dd3f175dc622e8c"><img src="../assets/perspective1.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 1:</strong> <em>the human eye is an optical system that converges light rays (light reflected from an object) to a focus point. As a result, by geometric construction, objects which are further away from our eyes, do appear smaller than those which are at close distance.</em></p>
<p>As a result, by geometric construction, objects which are further away from our eyes, appear smaller than those which are at a close distance (assuming all objects have the same size). Or to say it differently, an object appears smaller as we move away from it. Again this is the pure result of the way our eyes are designed. But because we are accustomed to seeing the world that way, it makes sense to produce images that have the same effect: something called the <strong>foreshortening effect</strong>. Cameras and photographic lenses were designed to produce images of that sort. More than simulating the laws of physics, photorealistic rendering, is also about simulating the way our visual system works. We need to produce images of the world on a flat surface, similar to the way images are created in our eyes (which is mostly the result of the way our eyes are designed - we are not too sure about how it works in the brain but this is not important for us).</p>
<p>How do we do that? A basic method consists of tracing lines from the corner of objects to the eye and finding the intersection of these lines with the surface of an imaginary canvas (a flat surface on which the image will be drawn, such as a sheet of paper or the surface of the screen) perpendicular to the line of sight (Figure 2).</p>
<p><a href="#R-image-895dae4c875acf854fadf832e45dafa4" class="lightbox-link"><img src="../assets/perspective2.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-895dae4c875acf854fadf832e45dafa4"><img src="../assets/perspective2.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 2:</strong> <em>to create an image of the box, we trace lines from the corners of the object to the eye. We then connect the points where these lines intersect an imaginary plane (the canvas) to recreate the edges of the cube. This is an example of perspective projection.</em></p>
<p>These intersection points can then be connected, to recreate the edges of the objects. The process by which a 3D point is projected onto the surface of the canvas (by the process we just described) is called <strong>perspective projection</strong>. Figure 3 shows what a box looks like when this technique is used to &ldquo;trace&rdquo; an image of that object on a flat surface (the canvas).</p>
<p><a href="#R-image-248cb29136deceaae7e9e3e52958259c" class="lightbox-link"><img src="../assets/perspective3.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-248cb29136deceaae7e9e3e52958259c"><img src="../assets/perspective3.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 3:</strong> <em>image of a cube created using perspective projection.</em></p>
<p>This sort of rendering in computer graphics is called a wireframe because only the edges of the objects are drawn. This image though is not photo-real. If the box was opaque, the front faces of the box (at most three of these faces) should occlude or hide the rear ones, which is not the case in this image (and if more objects were in the scene, they would potentially occlude each other). Thus, one of the problems we need to figure out in rendering is not only how we should be projecting the geometry onto the scene, but also how we should determine which part of the geometry is visible and which part is hidden, something known as the visibility problem (determining which surfaces and parts of surfaces are not visible from a certain viewpoint). This process in computer graphics is known under many names: <strong>hidden surface elimination</strong>, <strong>hidden surface determination</strong> (also known as hidden surface removal, <strong>occlusion culling</strong>, and <strong>visible surface determination</strong>. Why so many names? Because this is one of the first major problems in rendering, and for this particular reason, a lot of research was made in this area in the early ages of computer graphics (and a lot of different names were given to the different algorithms that resulted from this research). Because it requires finding out whether a given surface is hidden or visible, you can look at the problem in two different ways: do I design an algorithm that looks for hidden surfaces (and remove them), or do I design one in which I focus on finding the visible ones. Of course, this should produce the same image at the end but can lead to designing different algorithms (in which one might be better than the others).</p>
<p>The visibility problem can be solved in many different ways, but they generally fall within two main categories. In historical-chronological order:</p>
<ul>
<li>Rasterization,</li>
<li>Ray-tracing.</li>
</ul>
<p>Rasterization is not a common name, but for those of you who are already familiar with hidden surface elimination algorithms, it includes the z-buffer and painter&rsquo;s algorithms among others. Almost all graphics cards (GPUs) use an algorithm from this category (likely z-buffering). Both methods will be detailed in the next chapter.</p>
<h3 id="shading">Shading</h3>
<p>Even though we haven&rsquo;t explained how the visibility problem can be solved, let&rsquo;s assume for now that we know how to flatten a 3D scene onto a flat surface (using perspective projection) and determine which part of the geometry is visible from a certain viewpoint. This is a big step towards generating a photorealistic image but what else do we need? Objects are not only defined by their shape but also by their appearance (this time not in terms of how big they appear on the scene, but in terms of their look, color, texture, and how bright they are). Furthermore, objects are only visible to the human eye because light is bouncing off their surface. How can we define the appearance of an object? The appearance of an object can be defined as the way the material this object is made of, interacts with light itself. Light is emitted by light sources (such as the sun, a light bulb, the flame of a candle, etc.) and travels in a straight line. When it comes in contact with an object, two things might happen to it. It can either be absorbed by the object or it can be reflected in the environment. When light is reflected off the surface of an object, it keeps traveling (potentially in a different direction than the direction it came from initially) until it either comes in contact with another object (in which case the process repeats, light is either absorbed or reflected) or reach our eyes (when it reaches our eyes, the photoreceptors the surface of the eye is made of convert light into an electrical signal which is sent to the brain).</p>
<p><a href="#R-image-72b5543e0323f8c1ecd7e0e9b015db14" class="lightbox-link"><img src="../assets/lemon.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-72b5543e0323f8c1ecd7e0e9b015db14"><img src="../assets/lemon.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 4:</strong> <em>an object appears yellow under white light because it absorbs most of the blue light and reflects green and red light which combined to form a yellow color.</em></p>
<ul>
<li><strong>Absorption</strong> gives objects their unique color. White light (check the lesson on color in the section Introduction to Computer Graphics) is composed of all colors making up the visible spectrum. When white light strikes an object, some of these light colors are absorbed while others are reflected. Mixed, these reflected colors define the color of the object. Under sunlight, if an object appears yellow, you can assume that it absorbs blue light and reflects a combination of red and green light, which combined form the yellow color. A black object absorbs all light colors. A white object reflects them all. The color of an object is unique to the way the material this object is made of absorbs light (it is a unique property of that material).</li>
<li><strong>Reflection</strong>. We already know that an object reflects light colors which it doesn&rsquo;t absorb, but in which direction is this light reflected? It happens that the answer to this question is both simple and very complex. At the object level, light behaves no differently than a tennis ball when it bounces back from the surface of a solid object. It simply travels along a direction similar to the direction it came in but flipped around a vector perpendicular to the orientation of the surface at the impact point. In computer graphics, we call this direction a <strong>normal</strong>: the outgoing direction is a <strong>reflection</strong> of the incoming direction with respect to the normal. At the atomic level, when a photon interacts with an atom, the photon can either be absorbed or re-emitted by the atom in any new random direction. The re-emission of a photon by an atom is called <strong>scattering</strong>. We will speak about this term again in a very short while.</li>
</ul>
<p>In CG, we generally won&rsquo;t try to simulate the way light interacts with atoms, but the way it behaves at the object level. However, things are not that simple. Because if the maths involved in computing the new direction of a tennis ball bouncing off the surface of an object are simple, the problem is that surfaces at the microscopic level (not the atomic level) are generally not flat at all, which causes light to bounce in all sort of (almost random in some cases) directions. From the distance we generally look at common objects (a car, a pillow, a fruit), we don&rsquo;t see the microscopic structure of objects, although it has a considerable impact on the way it reflects light and thus the way they look. However, we are not going to represent objects at the microscopic level, for obvious reasons (the amount of geometry needed would simply not fit within the memory of any conventional or non-conventional for that matter, computer). What do we do then? The solution to this problem is to come up with another mathematical model, for simulating the way light interacts with any given material at the microscopic level. This, in short, is the role played by what we call a <strong>shader</strong> in computer graphics. A shader is an implementation of a mathematical model designed to simulate the way light interacts with matter at the microscopic level.</p>
<h3 id="light-transport">Light Transport</h3>
<p>Rendering is mostly about simulating the way light travels in space. Light is emitted from light sources, and is reflected off the surface of objects, and some of that light eventually reaches our eyes. This is how and why we see objects around us. As mentioned in the introduction to ray tracing, it is not very efficient to follow the path of light from a light source to the eye. When a photon hits an object, we do not know the direction this photon will have after it has been reflected off the surface of the object. It might travel towards the eyes, but since the eye is itself very small, it is more likely to miss it. While it&rsquo;s not impossible to write a program in which we simulate the transport of light as it occurs in nature (this method is called <strong>forward tracing</strong>), it is, as mentioned before, never done in practice because of its inefficiency.</p>
<p><a href="#R-image-53105ff1e1d2120f97c313e6e5b02902" class="lightbox-link"><img src="../assets/lighttransport.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-53105ff1e1d2120f97c313e6e5b02902"><img src="../assets/lighttransport.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 5:</strong> <em>in the real world, light travel travels from light sources (the sun, light bulbs, the flame of a candle, etc.) to the eye. This is called forward tracing (left). However, in computer graphics and rendering, it&rsquo;s more efficient to simulate the path of light the other way around, from the eye to the object, to the light source. This is called backward tracing.</em></p>
<p>A much more efficient solution is to follow the path of light, the other way around, from the eye to the light source. Because we follow the natural path of light backward, we call this approach <strong>backward tracing</strong>.</p>
<p>Both terms are sometimes swapped in the CG literature. Almost all renderers follow light from the eye to the emission source. Because in computer graphics, it is the &lsquo;default&rsquo; implementation, some people prefer to call this method, forward tracing. However, in Scratchapixel, we will use forward for when light goes from the source to the eye, and backward when we follow its path the other way around.</p>
<p>The main point here is that rendering is for the most part about simulating the way light propagates through space. This is not a simple problem, not because we don&rsquo;t understand it well, but because if we were to simulate what truly happens in nature, there would be so many photons (or light particles) to follow the path of, that it would take a very long time to get an image. Thus in practice, we follow the path of very few photons instead, just to keep the render time down, but the final image is not as accurate as it would be if the paths of all photons were simulated. Finding a good tradeoff between photo-realism and render time is the crux of rendering. In rendering, a light transport algorithm is an algorithm designed to simulate the way light travels in space to produce an image of a 3D scene that matches &ldquo;reality&rdquo; as closely as possible.</p>
<p>When light bounces off a diffuse surface and illuminates other objects around it, we call this effect <strong>indirect diffuse</strong>. Light can also be reflected off the surface of shiny objects, creating caustics (the disco ball effect). Unfortunately, it is very hard to come up with an algorithm capable of simulating all these effects at once (using a single light transport algorithm to simulate them all). It is in practice, often necessary to simulate these effects independently.</p>
<p>Light transport is central to rendering and is a very large field of research.</p>
<h3 id="summary-1">Summary</h3>
<p>In this chapter, we learned that rendering can essentially be seen as an essential two steps process:</p>
<ul>
<li>The perspective projection and visibility problem on one hand,</li>
<li>And the simulation of light (light transport) as well the simulation of the appearance of objects (shading) on the other.</li>
</ul>
<blockquote>
<details>
Have you ever heard the term **graphics or rendering pipeline**? The term is more often used in the context of real-time rendering APIs (such as OpenGL, DirectX, or Metal). The rendering process as explained in this chapter can be decomposed into at least two steps, visibility, and shading. Both steps though can be decomposed into smaller steps or stages (which is the term more commonly used). Steps or stages are generally executed in sequential order (the input of any given stage generally depends on the output of the preceding stage). This sequence of stages forms what we call the rendering pipeline.
</details>
</blockquote>
<p>You must always keep this distinction in mind. When you study a particular technique always try to think whether it relates to one or the other. Most lessons from this section (and the advanced rendering section) fall within one of these categories:</p>
<p><a href="#R-image-9d99deffd71784c524f55f8c1babb2c2" class="lightbox-link"><img src="../assets/20240216115455.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-9d99deffd71784c524f55f8c1babb2c2"><img src="../assets/20240216115455.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p>We will briefly detail both steps in the next chapters.</p>
<h2 id="perspective-projection">Perspective Projection</h2>
<p>In the previous chapter, we mentioned that the rendering process could be looked at as a two steps process:</p>
<ul>
<li>projecting 3D shapes on the surface of a canvas and determining which part of these surfaces are visible from a given point of view,</li>
<li>simulating the way light propagates through space, which combined with a description of the way light interacts with the materials objects are made of, will give these objects their final appearance (their color, their brightness, their texture, etc.).</li>
</ul>
<p>In this chapter, we will only review the first step in more detail, and more precisely explain how each one of these problems (projecting the objects&rsquo; shape on the surface of the canvas and the visibility problem) is typically solved. While many solutions may be used, we will only look at the most common ones. This is just an overall presentation. Each method will be studied in a separate lesson and an implementation of these algorithms provided (in a self-contained C++ program).</p>
<h3 id="going-from-3d-to-2d-the-projection-matrix">Going from 3D to 2D: the Projection Matrix</h3>
<p><a href="#R-image-015d6c3300f2145690f2cc4ba3a9e9b7" class="lightbox-link"><img src="../assets/perspective4.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-015d6c3300f2145690f2cc4ba3a9e9b7"><img src="../assets/perspective4.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 1:</strong> <em>to create an image of a cube, we just need to extend lines from the corners of the object towards the eye and find the intersection of these lines with a flat surface (the canvas) perpendicular to the line of sight.</em></p>
<p>An image is just a representation of a 3D scene on a flat surface: the surface of a canvas or the screen. As explained in the previous chapter, to create an image that looks like reality to our brain, we need to simulate the way an image of the world is formed in our eyes. The principle is quite simple. We just need to extend lines from the corners of the object towards the eye and find the intersection of these lines with a flat surface perpendicular to the line of sight. By connecting these points to draw the edges of the object, we get a <strong>wireframe</strong> representation of the scene.</p>
<blockquote>
<details>
It is important to note, that this sort of construction is in a way a completely arbitrary way of flattening a three-dimensional world onto a two-dimensional surface. The technique we just described gives us what is called in drawing, a one-point perspective projection, and this is generally how we do things in CG because this is how the eyes and also cameras work (cameras were designed to produce images similar to the sort of images our eyes create). But in the art world, nothing stops you from coming up with totally different rules. You can in particular get images with several (two, three, four) points perspective.
</details>
</blockquote>
<p>One of the main important visual properties of this sort of projection is that an object gets smaller as it moves further away from the eye (the rear edges of a box are smaller than the front edges). This effect is called <strong>foreshortening</strong>.</p>
<p><a href="#R-image-d9113ed514911886ed1339b10f29fd07" class="lightbox-link"><img src="../assets/frustum2.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-d9113ed514911886ed1339b10f29fd07"><img src="../assets/frustum2.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 2:</strong> the line of sight passes through the center of the canvas.</p>
<p><a href="#R-image-50c031dbe921f24c2aab21dd66593b68" class="lightbox-link"><img src="../assets/frustum1.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-50c031dbe921f24c2aab21dd66593b68"><img src="../assets/frustum1.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 3:</strong> <em>the size of the canvas can be changed. Making it smaller reduces the field of view.</em></p>
<p>There are two important things to note about this type of projection. First, the eye is in the center of the canvas. In other words, the line of sight always passes through the middle of the image (figure 2). Note also that the size of the canvas itself is something we can change. We can more easily understand what the impact of changing the size of the canvas has if we draw the viewing frustum (figure 3). The <strong>frustum</strong> is the pyramid defined by tracing lines from each corner of the canvas toward the eye, and extending these lines further down into the scene (as far as the eye can see). It is also referred to as the viewing frustum or viewing volume. You can easily see that the only objects visible to the camera are those which are contained within the volume of that pyramid. By changing the size of the canvas we can either extend that volume or make it smaller. The larger the volume the more of the scene we see. If you are familiar with the concept of focal length in photography, then you will have recognized that this has the same effect as changing the focal length of photographic lenses. Another way of saying this is that by changing the size of the canvas, we change the field of view.</p>
<p><a href="#R-image-e09e372b29777bf018d0394df113f827" class="lightbox-link"><img src="../assets/ortho.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-e09e372b29777bf018d0394df113f827"><img src="../assets/ortho.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 4:</strong> <em>when the canvas becomes infinitesimally small, the lines of the frustum become orthogonal to the canvas. We then get what we call an orthographic projection. The game SimCity uses a form of orthographic view which gives it a unique look.</em></p>
<p>Something interesting happens when the canvas becomes infinitesimally small: the lines forming the frustum, end up parallel to each other (they are orthogonal to the canvas). This is of course impossible in reality, but not impossible in the virtual world of a computer. In this particular case, you get what we call an <strong>orthographic projection</strong>. It&rsquo;s important to note that orthographic projection is a form of perspective projection, only one in which the size of the canvas is virtually zero. This has for effect to cancel out the <strong>foreshortening effect</strong>: the size of the edges of objects are preserved when projected to the screen.</p>
<p><a href="#R-image-f518a9a186b32485a572bb275bd2d992" class="lightbox-link"><img src="../assets/projection.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-f518a9a186b32485a572bb275bd2d992"><img src="../assets/projection.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 5:</strong> <em>P&rsquo; is the projection of P on the canvas. The coordinates of P&rsquo; can easily be computed using the property of similar triangles.</em></p>
<p>Geometrically, computing the intersection point of these lines with the screen is incredibly simple. If you look at the adjacent figure (where P is the point projected onto the canvas, and P&rsquo; is this projected point), you can see that the angle \(\angle ABC\) and \(\angle AB&rsquo;C&rsquo;\) is the same. A is defined as the eye, AB is the distance of the point P along the z-axis (P&rsquo;s z-coordinate), and BC is the distance of the point P along the y-axis (P&rsquo;s y coordinate). B&rsquo;C&rsquo; is the y coordinate of P&rsquo;, and AB&rsquo; is the z-coordinate of P&rsquo; (and also the distance of the eye to the canvas). When two triangles have the same angle, we say that they are <strong>similar</strong>. Similar triangles have an interesting property: the ratio of the lengths of their corresponding sides is constant. Based on this property, we can write that:</p>

<span class="math align-center">$$
{ BC \over AB } = { B'C' \over AB' }
$$</span><p>If we assume that the canvas is located 1 unit away from the eye (in other words that AB&rsquo; equals 1 (this is purely a convention to simplify this demonstration), and if we substitute AB, BC, AB&rsquo; and B&rsquo;C&rsquo; with their respective points&rsquo; coordinates, we get:</p>

<span class="math align-center">$$
{ BC \over AB } = { B'C' \over 1 } \rightarrow P'.y = { P.y \over P.z }.
$$</span><p>In other words, to find the y-coordinate of the projected point, you simply need to divide the point y-coordinate by its z-coordinate. The same principle can be used to compute the x coordinate of P&rsquo;:</p>

<span class="math align-center">$$
P'.x = { P.x \over P.z }.
$$</span><p>This is a very simple yet extremely important relationship in computer graphics, known as the <strong>perspective divide</strong> or z-divide (if you were on a desert island and needed to remember something about computer graphics, that would probably be this equation).</p>
<p>In computer graphics, we generally perform this operation using what we call a <strong>perspective projection matrix</strong>. As its name indicates, it&rsquo;s a matrix that when applied to points, projects them to the screen. In the next lesson, we will explain step by step how and why this matrix works, and learn how to build and use it.</p>
<p>But wait! The problem is that whether you need the perspective projection depends on the technique you use to sort out the visibility problem. Anticipating what we will learn in the second part of this chapter, algorithms for solving the visibility problem come into two main categories:</p>
<ul>
<li>Rasterization,</li>
<li>Ray-tracing.</li>
</ul>
<p>Algorithms of the first category rely on projecting P onto the screen to compute P&rsquo;. For these algorithms, the perspective projection matrix is therefore needed. In ray tracing, rather than projecting the geometry onto the screen, we trace a ray passing through P&rsquo; and look for P. We don&rsquo;t need to project P anymore with this approach since we already know P&rsquo;, which means that in ray tracing, the perspective projection is technically not needed (and therefore never used).</p>
<blockquote>
<details>
We will study the two algorithms in detail in the next chapters and the next lessons. However, it is important to understand the difference between the two and how they work at this point. As explained before, the geometry needs to be projected onto the surface of the canvas. To do so, P is projected along an "implicit" line (implicit because we never really need to build this line as we need to with ray tracing) connecting P to the eye. You can see the process as if you were moving a point along that line from P to the eye until it lies on the canvas. That point would be P'. In this approach, you know P, but you don't know P'. You compute it using the projection approach. But you can also look at the problem the other way around. You can wonder whether, for any point on the canvas (say P' - which by default we will assume is in the center of the pixel), there is a point P on the surface of the geometry that projects onto P'. The solution to this problem is to explicitly this time create a ray from the eye to P', extend or project this ray down into the scene, and find out if this ray intersects any 3D geometry. If it does, then the intersection point is P. Hopefully, you can now see more distinctively the difference between rasterization (we know P, we compute P') and ray tracing (we know P', we look for P).
</details>
</blockquote>
<p>The advantage of the rasterization approach over ray tracing is mainly speed. Computing the intersection of rays with geometry is a computationally expensive operation. This intersection time also grows linearly with the amount of geometry contained in the scene, as we will see in one of the next lessons. On the other hand, the projection process is incredibly simple, relies on basic math operations (multiplications, divisions, etc.), and can be aggressively optimized (especially if special hardware is designed for this purpose which is the case with GPUs). Graphics cards are almost all using an algorithm based on the rasterization approach (which is one of the reasons they can render 3D scenes so quickly, at interactive frame rates). When real-time rendering APIs such as OpenGL or DirectX are used, the projection matrix needs to be dealt with. Even if you are only interested in ray tracing, you should know about it for at least a historical reason: it is one of the most important techniques in rendering and the most commonly used technique for producing real-time 3D computer graphics. Plus, it is likely at some point that you will have to deal with the GPU anyway, and real-time rendering APIs do not compute this matrix for you. You will have to do it yourself.</p>
<blockquote>
<details>
The concept of rasterization is really important in rendering. As we learned in this chapter, the projection of P onto the screen can be computed by dividing the point's coordinates x and y by the point's z-coordinate. As you may guess, all initial coordinates are real numbers - floats for instance - thus P' coordinates are also real numbers. However pixel coordinates need to be integers, thereby, to store the color of P's in the image, we will need to convert its coordinates to pixel coordinates - in other words from floats to integers. We say that the point's coordinates are converted from screen space to raster space. More information can be found on this process in the lesson on rays and cameras.
</details>
</blockquote>
<p>The next three lessons are devoted to studying the construction of the orthographic and perspective matrix, and how to use them in OpenGL to display images and 3D geometry.</p>
<h2 id="the-visibility-problem">The Visibility Problem</h2>
<p>We already explained what the visibility problem is in the previous chapters. To create a photorealistic image, we need to determine which part of an object is visible from a given viewpoint. The problem is that when we project the corners of the box for example and connect the projected points to draw the edges of the box, all faces of the box are visible. However, in reality, only the front faces of the box would be visible, while the rear ones would be hidden.</p>
<p>In computer graphics, you can solve this problem using principally two methods: <strong>ray tracing</strong> and <strong>rasterization</strong>. We will quickly explain how they work. While it&rsquo;s hard to know whether one method is older than the other, rasterization was far more popular in the early days of computer graphics. Ray tracing is notoriously more computationally expensive (and uses more memory) than rasterization, and thus is far slower in comparison. Computers back then were so slow (and had so little memory), that rendering images using ray tracing was not considered a viable option, at least in a production environment (to produce films for example). For this reason, almost every renderer used rasterization (ray tracing was generally limited to research projects). However, for reasons we will explain in the next chapter, ray tracing is way better than rasterization when it comes to simulating effects such as reflections, soft shadows, etc. In summary, it&rsquo;s easier to create photo-realistic images with ray tracing, only it takes longer compared to rendering geometry using rasterization which in turn, is less adapted than ray tracing to simulate realistic shading and light effects. We will explain why in the next chapter. Real-time rendering APIs and GPUs are generally using rasterization because speed in real-time is obviously what determines the choice of the algorithm. What was true for ray tracing in the 80s and 90s is however not true today. Computers are now so powerful, that ray tracing is used by probably every offline renderer today (at least, they propose a hybrid approach in which both algorithms are implemented). Why? Because again it&rsquo;s the easiest way of simulating important effects such as sharp and glossy reflections, soft shadows, etc. As long as the speed is not an issue, it is superior in many ways to rasterization (making ray tracing work efficiently though still requires a lot of work). Pixar&rsquo;s PhotoRealistic RenderMan, the renderer Pixar developed to produce many of its first feature films (Toys Story, Nemo, Bug&rsquo;s Life) was based on a rasterization algorithm (the algorithm is called REYES; it stands for Renders Everything You Ever Saw. It is by far considered one of the best visible surface determination algorithms ever conceived - The GPU rendering pipeline has many similarities with REYES). But their current renderer called RIS is now a pure ray tracer. Introducing ray tracing allowed the studio to greatly push the realism and complexity of the images it produced over the years.</p>
<h3 id="rasterisation-to-solve-the-visibility-problem-how-does-it-work">Rasterisation to Solve the Visibility Problem: How Does it Work?</h3>
<p>We hopefully clearly explained already the difference between rasterization and ray tracing (read the previous chapter). However let&rsquo;s repeat, that we can look at the rasterization approach as if we were moving a point along a line connecting P, a point on the surface of the geometry, to the eye until it &ldquo;lies&rdquo; on the surface of the canvas. Of course, this line is only implicit, we never really need to construct it, but this is how intuitively we can interpret the projection process.</p>
<p><a href="#R-image-0d27419e5ce0d584f4d1b9bcec1234b3" class="lightbox-link"><img src="../assets/projection3.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-0d27419e5ce0d584f4d1b9bcec1234b3"><img src="../assets/projection3.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 1:</strong> the projection process can be seen as if the point we want to project was moved down along a line connecting the point or the vertex itself to the eye. We can stop moving the point along that line when it lies on the plane of the canvas. Obviously, we don&rsquo;t &ldquo;slide&rdquo; the point along this line explicitly, but this is how the projection process can be interpreted.</p>
<p><a href="#R-image-fcfe4e00b20b0f113636962e2e46da62" class="lightbox-link"><img src="../assets/projection2.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-fcfe4e00b20b0f113636962e2e46da62"><img src="../assets/projection2.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 2:</strong> <em>several points in the scene may project to the same point on the scene. The point visible to the camera is the one closest to the eye along the ray on which all points are aligned.</em></p>
<p>Remember that what we need to solve here is the visibility problem. In other words, there might be situations in which several points in the scene, P, P1, P2, etc. project onto the same point P&rsquo; onto the canvas (remember that the canvas is also the surface of the screen). However, the only point that is visible through the camera is the point along the line connecting the eye to all these points, which is the closest to the eye, as shown in Figure 2.</p>
<p>To solve the visibility problem, we first need to express P&rsquo; in terms of its position in the image: what are the coordinates of the pixel in the image, P&rsquo; falls onto? Remember that the projection of a point to the surface of the canvas gives another point P&rsquo; whose coordinates are real. However, P&rsquo; also necessarily falls within a given pixel of our final image. So how do we go from expressing P&rsquo;s in terms of their position on the surface of the canvas, to defining it in terms of their position in the final image (the coordinates of the pixel in the image, P&rsquo; falls onto)? This involves a simple change of coordinate systems.</p>
<ul>
<li>The coordinate system in which the point is originally defined is called <strong>screen space</strong> (or image space). It is defined by an origin that is located in the center of the canvas. All axes of this two-dimensional coordinate system have unit length (their length is 1). Note that the x or y coordinate of any point defined in this coordinate system can be negative if it lies to the left of the x-axis (for the x-coordinate) or below the y-axis (for the y-coordinate).</li>
<li>The coordinate system in which points are defined with respect to the grid formed by the pixels of the image, is called <strong>raster space</strong>. Its origin is generally located in the upper-left corner of the image. Its axes also have unit length and a pixel is considered to be one unit length in this coordinate system. Thus, the actual size of the canvas in this coordinate system is given by the image&rsquo;s vertical (height) and horizontal (width) dimensions (which are expressed in terms of pixels).</li>
</ul>
<p><a href="#R-image-3f97ae5cf36a4ceab9ad0299b2258510" class="lightbox-link"><img src="../assets/screentoraster.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-3f97ae5cf36a4ceab9ad0299b2258510"><img src="../assets/screentoraster.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 3:</strong> <em>computing the coordinate of a point on the canvas in terms of pixel values, requires to transform the points&rsquo; coordinates from screen to NDC space, and NDC space to raster space.</em></p>
<p>Converting points from screen space to raster space is simple. Because the coordinates P&rsquo; expressed in raster space can only be positive, we first need to normalize P&rsquo;s original coordinates. In other words, convert them from whatever range they are originally in, to the range [0, 1] (when points are defined that way, we say they are defined in NDC space. NDC stands for Normalized Device Coordinates). Once converted to NDC space, converting the point&rsquo;s coordinates to raster space is trivial: just multiply the normalized coordinates by the image dimensions, and round the number off to the nearest integer value (pixel coordinates are always round numbers, or integers if you prefer). The range P&rsquo; coordinates are originally in, depends on the size of the canvas in screen space. For the sake of simplicity, we will just assume that the canvas is two units long in each of the two dimensions (width and height), which means that P&rsquo; coordinates in screen space, are in the range [-1, 1]. Here is the pseudo-code to convert P&rsquo;s coordinates from screen space to raster space:</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="ln"> 1</span><span class="cl"><span class="kt">int</span> <span class="n">width</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">64</span><span class="p">;</span>  <span class="c1">//dimension of the image in pixels 
</span></span></span><span class="line"><span class="ln"> 2</span><span class="cl"><span class="c1"></span><span class="n">Vec3f</span> <span class="n">P</span> <span class="o">=</span> <span class="n">Vec3f</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">);</span> 
</span></span><span class="line"><span class="ln"> 3</span><span class="cl"><span class="n">Vec2f</span> <span class="n">P_proj</span><span class="p">;</span> 
</span></span><span class="line"><span class="ln"> 4</span><span class="cl"><span class="n">P_proj</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">P</span><span class="p">.</span><span class="n">x</span> <span class="o">/</span> <span class="n">P</span><span class="p">.</span><span class="n">z</span><span class="p">;</span>  <span class="c1">//-0.1 
</span></span></span><span class="line"><span class="ln"> 5</span><span class="cl"><span class="c1"></span><span class="n">P_proj</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">P</span><span class="p">.</span><span class="n">y</span> <span class="o">/</span> <span class="n">P</span><span class="p">.</span><span class="n">z</span><span class="p">;</span>  <span class="c1">//0.2 
</span></span></span><span class="line"><span class="ln"> 6</span><span class="cl"><span class="c1">// convert from screen space coordinates to normalized coordinates
</span></span></span><span class="line"><span class="ln"> 7</span><span class="cl"><span class="c1"></span><span class="n">Vec2f</span> <span class="n">P_proj_nor</span><span class="p">;</span> 
</span></span><span class="line"><span class="ln"> 8</span><span class="cl"><span class="n">P_proj_nor</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">P_proj</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span>  <span class="c1">//(-0.1 + 1) / 2 = 0.45 
</span></span></span><span class="line"><span class="ln"> 9</span><span class="cl"><span class="c1"></span><span class="n">P_proj_nor</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">P_proj</span><span class="p">.</span><span class="n">y</span> <span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span>  <span class="c1">//(1 - 0.2) / 2 = 0.4 
</span></span></span><span class="line"><span class="ln">10</span><span class="cl"><span class="c1">// finally, convert to raster space
</span></span></span><span class="line"><span class="ln">11</span><span class="cl"><span class="c1"></span><span class="n">Vec2i</span> <span class="n">P_proj_raster</span><span class="p">;</span> 
</span></span><span class="line"><span class="ln">12</span><span class="cl"><span class="n">P_proj_raster</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="p">)(</span><span class="n">P_proj_nor</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">width</span><span class="p">);</span> 
</span></span><span class="line"><span class="ln">13</span><span class="cl"><span class="n">P_proj_raster</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="p">)(</span><span class="n">P_proj_nor</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">height</span><span class="p">);</span> 
</span></span><span class="line"><span class="ln">14</span><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">P_proj_raster</span><span class="p">.</span><span class="n">x</span> <span class="o">==</span> <span class="n">width</span><span class="p">)</span> <span class="n">P_proj_raster</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">width</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span> 
</span></span><span class="line"><span class="ln">15</span><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">P_proj_raster</span><span class="p">.</span><span class="n">y</span> <span class="o">==</span> <span class="n">height</span><span class="p">)</span> <span class="n">P_proj_raster</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">height</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span> 
</span></span></code></pre></div><p>This conversion process is explained in detail in the lesson <a href="../../../../../lessons/3d-basic-rendering/3d-viewing-pinhole-camera/">3D Viewing: the Pinhole Camera Model</a>.</p>
<p>There are a few things to notice in this code. First that the original point P, the projected point in screen space, and NDC space all use the Vec3f or Vec2f types in which the coordinates are defined as real (floats). However, the final point in raster space uses the Vec2i type in which coordinates are defined as integers (the coordinate of a pixel in the image). Arrays in programming, are 0-indexed, thereby, the coordinates of a point in raster point should never be greater than the width of the image minus one or the image height minus one. However, this may happen when P&rsquo;s coordinates in screen space are exactly 1 in either dimension. The code checks this case (lines 14-15) and clamps the coordinates to the right range if it happens. Also, the origin of the NDC space coordinate is located in the lower-left corner of the image, but the origin of the raster space system is located in the upper-left corner (see figure 3). Therefore, the y coordinate needs to be inverted when converted from NDC to raster space (check the difference between lines 8 and 9 in the code).</p>
<p>But why do we need this conversion? To solve the visibility problem we will use the following method:</p>
<ul>
<li>
<p>Project all points onto the screen.</p>
<ul>
<li>
<p>For each projected point, convert P&rsquo;s coordinates from screen space to raster space.</p>
</li>
<li>
<p>Find the pixel the point maps to (using the projected point raster coordinates), and store the distance of that point to the eye, in a special list of points (called the depth list), maintained by that pixel.</p>
</li>
</ul>
</li>
</ul>
<blockquote>
  <details>
  _You say, project all points onto the screen. How do we find these points in the first place?"_
  Very good question. Technically, we would break down the triangles or the polygons objects are made of, into smaller geometry elements no bigger than a pixel when projected onto the screen. In real-time APIs (OpenGL, DirectX, Vulkan, Metal, etc.) this is what we generally refer to as fragments. Check the lesson on the REYES algorithm in this section to learn how this works in more detail.
  </details>  
</blockquote>
<ul>
<li>At the end of the process, sort the points in the list of each pixel, by order of increasing distance. As a result of this process, the point visible for any given pixel in the image is the first point from that pixel&rsquo;s list.</li>
</ul>
<blockquote>
<details>
_Why do points need to be sorted according to their depth?_
The list needs to be sorted because points are not necessarily ordered in depth when projected onto the screen. Assuming you insert points by adding them at the top of the list, you may project a point B further from the eye than a point A, after you projected A. In which case B will be the first point in the list, even though its distance to the eye, is greater than the distance to A. Thus sorting is required.
</details>
</blockquote>
<p>An algorithm based on this approach is called a <strong>depth sorting algorithm</strong> (a self-explanatory name). <strong>The concept of depth ordering is the base of all rasterization algorithms</strong>. Quite a few exist among the most famous of which are:</p>
<ul>
<li>the z-buffering algorithm. This is probably the most commonly used one from this category. The REYES algorithm which we present in this section implements the z-buffer algorithm. It is very similar to the technique we described in which points on the surfaces of objects (objects are subdivided into very small surfaces or fragments which are then projected onto the screen), are projected onto the screen and stored into depth lists.</li>
<li>the Painter algorithm</li>
<li>Newell&rsquo;s algorithm</li>
<li>&hellip; (list to be extended)</li>
</ul>
<p>Keep in mind that while this may sound like old fashion to you, all graphics cards are using one implementation of the z-buffer algorithm, to produce images. These algorithms (at least z-buffering) are still commonly used today.</p>
<blockquote>
<details>
Why do we need to keep a list of points? Storing the point with the shortest distance to the eye shouldn't require storing all the points in a list. Indeed, you could very well do the following thing:
<ul>
<li>For each pixel in the image, set the variable z to infinity.</li>
<li>For each point in the scene.
<ul>
<li>Project the point and compute its raster coordinates</li>
<li>If the distance from the current point to the eye z&rsquo; is smaller than the distance z stored in the pixel the point projects to, then update z with z&rsquo;. If z&rsquo; is greater than z, then the point is located further away from the point currently stored for that pixel.</li>
</ul>
</li>
</ul>
<p>You can see that, you can get the same result without having to store a list of visible points and sorting them out at the end. So why did we use one? We used one because, in our example, we just assume that all points in the scene were opaque. But what happens if they are not fully opaque? If several semi-transparent points project to the same pixel, they may be visible throughout each other. In this particular case, it is necessary to keep track of all the points visible through that particular pixel, sort them out by distance, and use a special compositing technique (we will learn about this in the lesson on the REYES algorithm) to blend them correctly.</p>
</details>
</blockquote>
<h3 id="ray-tracing-to-solve-the-visibility-problem-how-does-it-work">Ray Tracing to Solve the Visibility Problem: How Does It Work?</h3>
<p><a href="#R-image-328e9f8ef973e34401bf6d3c58e432de" class="lightbox-link"><img src="../assets/raytracing.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-328e9f8ef973e34401bf6d3c58e432de"><img src="../assets/raytracing.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 4:</strong> <em>in raytracing, we explicitly trace rays from the eye down into the scene. If the ray intersects some geometry, the pixel the ray passes through takes the color of the intersected object.</em></p>
<p>With rasterization, points are projected onto the screen to find their respective position on the image plane. But we can look at the problem the other way around. Rather than going from the point to the pixel, we can start from the pixel and convert it into a point on the image plane (we take the center of the pixel and convert its coordinates defined in raster space to screen space). This gives us P&rsquo;. We can then trace a ray starting from the eye, passing through P&rsquo;, and extend it down into the scene (by default we will assume that P&rsquo; is the center of the pixel). If we find that the ray intersects an object, then we know that the point of intersection P is the point visible through that pixel. In short, ray tracing is a method to solve the point&rsquo;s visibility problem, by the mean of explicitly tracing rays from the eye down into the scene.</p>
<p>Note that in a way, ray tracing and rasterization are a reflection of each other. They are based on the same principle, but ray tracing is going from the eye to the object, while rasterization goes from the object to the eye. While they make it possible to find which point is visible for any given pixel in the image (they give the same result in that respect), implementing them requires solving very different problems. Ray tracing is more complicated in a way because it requires solving the ray-geometry intersection problem. Do we even have a way of finding the intersection of a ray with geometry? While it might be possible to find a way of computing whether or not a ray intersects a sphere, can we find a similar method to compute the intersection of a ray with a cone for instance? And what about another shape, and what about NURBS, subdivision surfaces, and implicit surfaces? As you can see, ray tracing can be used as long as a technique exists to compute the intersection of a ray with any type of geometry a scene might contain (or your renderer might support).</p>
<p>Over the years, a lot of research was put into efficient ways of computing the intersection of rays with the simplest of all possible shapes - the triangle - but also directly ray tracing other types of geometry: NURBS, implicit surfaces, etc. However, one possible alternative to supporting all geometry types is to convert all geometry to a single geometry representation before the rendering process starts, and have the renderer only test the intersection of rays with that one geometry representation. Because triangles are an ideal rendering primitive, most of the time, all geometry is converted to triangles meshes, which means that rather than implementing a ray-object intersection test per geometry type, you only need to test for the intersection of rays with triangles. This has many advantages:</p>
<ul>
<li>First as suggested before, the triangle has many properties that make it very attractive as a geometry primitive. It&rsquo;s co-planar, a triangle is indivisible (as creating more faces by connecting the existing vertices, as you would for faces having at least four or more vertices), but it can easily be subdivided into more triangles. Finally, the math for computing the barycentric coordinates of a triangle (which is used in texturing) is simple and robust.</li>
<li>Because triangles are a good geometry primitive, a lot of research was done to find the best possible ray-triangle intersection test. What is a good ray triangle intersection algorithm? It needs to be fast (get to the result using as few operations as possible). It needs to use the least memory possible (some algorithms are more memory-hungry than others because they require storing precomputed variables on the triangle geometry). And it also needs to be robust (floating-point arithmetic issues are hard to avoid).</li>
<li>From a coding point of view, supporting one single routine is far more advantageous than having to code many routines to handle all geometry types. Supporting triangles only simplifies the code in many places but also allows to design code that works best with triangles in general. This is particularly true when it comes to acceleration structures. Computing the intersection of rays with geometry is by far the most expensive operation in a ray tracer. The time it takes to test the intersection with all geometry in the scene grows linearly with the amount of geometry the scene contains. As soon as the scene contains even just hundreds of such primitives it becomes necessary to implement strategies to quickly discard sections of the scene, which we know have no chances to be intersected by the ray, and test for only subsections of the scene that the ray will potentially intersect. These strategies save a considerable amount of time and are generally based on acceleration structures. We will study acceleration structures in the section devoted to ray tracing techniques. Also, it&rsquo;s worth noticing that specially designed hardware has been already built in the past, to handle the ray-triangle intersection test specifically, allowing complex scenes to run near real-time using ray tracing. It&rsquo;s quite obvious that in the future, graphics cards will natively support the ray-triangle intersection test and that video games will evolve towards ray tracing.</li>
</ul>
<h3 id="comparing-rasterization-and-ray-tracing">Comparing rasterization and ray-tracing</h3>
<p><a href="#R-image-ef68de0230b1ea74d331e78514f26c94" class="lightbox-link"><img src="../assets/gridaccel.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-ef68de0230b1ea74d331e78514f26c94"><img src="../assets/gridaccel.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 5:</strong> <em>the principle of acceleration structure consists of dividing space into sub-regions. As the ray travels from one sub-region to the next, we only need to check for a possible intersection with the geometry contained in the current sub-region. Instead of testing all the objects in the scene, we can only test for those contained in the sub-regions the ray passes through. This leads to potentially saving a lot of ray-geometry intersection tests which are costly.</em></p>
<p>We already talked a few times about the difference between ray tracing and rasterization. Why would you choose one or the other? As mentioned before, to sort the visibility problem, rasterization is faster than ray tracing. Why is that? Converting geometry to make it work with the rasterization algorithm takes eventually some time, but projecting the geometry itself is very fast (it just takes a few multiplications, additions, and divisions). In comparison, computing the intersection of a ray with geometry requires far more instructions and is, therefore, more expensive. The main difficulty with ray tracing is that render time increases linearly with the amount of geometry the scene contains. Because we have to check whether any given ray intersects any of the triangles in the scene, the final cost is then the number of triangles multiplied by the cost of a single ray-triangle intersection test. Hopefully, this problem can be alleviated by the use of an <strong>acceleration structure</strong>. The idea behind acceleration structures is that space can be divided into subspaces (for instance you can divide a box containing all the geometry to form a grid - each cell of that grid represents a sub-space of the original box) and that objects can be sorted depending on the sub-space they fall into. This idea is illustrated in figure 5.</p>
<p>If these sub-spaces are significantly larger than the objects&rsquo; average size, then it is likely that a subspace will contain more than one object (of course it all depends on how they are organized in space). Instead of testing all objects in the scene, we can first test if a ray intersects a given subspace (in other words, if it passes through that sub-space). If it does, we can then test if the ray intersects any of the objects it contains, but if it doesn&rsquo;t, we can then skip the ray-intersection test for all these objects. This leads to only testing a subset of the scene&rsquo;s geometry, which is saving time.</p>
<p>If acceleration structures can be used to accelerate ray tracing then isn&rsquo;t ray tracing superior to rasterization? Yes and no. First, it is still generally slower, but using an acceleration structure raises a lot of new problems.</p>
<ul>
<li>First building this structure takes time, which means the render can&rsquo;t start until it&rsquo;s built: this generally never takes more than a few seconds, but, if you intend to use ray tracing in a real-time application, then these few seconds are already too much (the acceleration structures needs to be built for every rendered frame if the geometry changes from frame to frame).</li>
<li>Second, an acceleration structure potentially takes a lot of memory. This all depends on the scene complexity, however, because a good chunk of the memory needs to be used for the acceleration structure, this means that less is available for doing other things, particularly storing geometry. In practice, this means you can potentially render less geometry with ray tracing than with rasterization.</li>
<li>Finally finding a good acceleration structure is very difficult. Imagine that you have one triangle on one side of the scene and all the other triangles stuck together in a very small region of space. If we build a grid for this scene many of the cells will be empty but the main problem is that when a ray traverses the cell containing the cluster of triangles, we will still need to perform a lot of intersection tests. Saving one test over the hundreds that may be required, is negligible and clearly shows that a grid as an acceleration structure for that sort of scene is not a good choice. As you can see, the efficiency of the acceleration structure depends very much on the scene, and the way objects are scattered: are object smalls or large, is it a mix of small and large objects, are objects uniformly distributed over space or very unevenly distributed? Is the scene a combination of any of these options?</li>
</ul>
<p><a href="#R-image-87de6a5356bc654fff9c7f730f74e6a8" class="lightbox-link"><img src="../assets/gridaccel2.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-87de6a5356bc654fff9c7f730f74e6a8"><img src="../assets/gridaccel2.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p>Many different acceleration structures have been proposed and they all have as you can guess strengths and weaknesses, but of course, some of them are more popular than others. You will find many lessons devoted to this particular topic in the section devoted to Ray Tracing Techniques.</p>
<p>From reading all this you may think that all problems are with ray tracing. Well, ray tracing is popular for a reason. First, in its principle, it is incredibly simple to implement. We showed in the first lesson of this section that a very basic raytracer can be written in no more than a few hundred lines of code. In reality, we could argue that it wouldn&rsquo;t take much more code to write a renderer based on the rasterization algorithm, but still, the concept of ray tracing seems to be easier to code, as maybe it is a more natural way of thinking of the process of making an image of a 3D scene. But far more importantly, it happens that if you use ray tracing, computing effects such as reflection or soft shadow which play a critical role in the photo-realism of an image, are just straightforward to simulate in ray tracing, and very hard to simulate if you use rasterization. To understand why, we first need to look at shading and light transport in more detail, which is the topic of our next chapter.</p>
<blockquote>
<p>Rasterization is fast but needs cleverness to support complex visual effects. Ray tracing supports complex visual effects but needs cleverness to be fast - David Luebke (NVIDIA).</p>
</blockquote>
<blockquote>
<p>With rasterization it is easy to do it very fast, but hard to make it look good. With ray tracing it is easy to make it look good, but very hard to make it fast.</p>
</blockquote>
<h3 id="summary-2">Summary</h3>
<p>In this chapter, we only look at using ray tracing and rasterization as two possible ways of solving the visibility problem. Rasterisation is still the method by which graphics cards render 3D scenes. Rasterisation is still faster compared to ray tracing when it comes to using one algorithm or the other to solve the visibility problem. You can accelerate ray tracing through with an acceleration structure, however, acceleration structures come with their own set of issues: it&rsquo;s hard to find a good acceleration structure, one that performs well regardless of the scene configuration (number of primitives to render, their sizes and their distribution in space). They also require extra memory and building them takes time.</p>
<p>It is important to appreciate that at this stage, ray tracing does not have any definite advantages over rasterization. However, ray tracing is better than rasterization to simulate light or shading effects such as soft shadows or reflections. When we say better we mean that is more straightforward to simulate them with ray tracing than it is with rasterization, which doesn&rsquo;t mean at all these effects can&rsquo;t be simulated with rasterization. It just generally only requires more work. We insist on this point because there is a common misbelief regarding the fact that effects such as reflections for example can&rsquo;t be done with rasterization, which is why ray tracing is used. It is simply not true. However, one might think about using a hybrid approach in which rasterization is used for the visibility surface elimination step, and ray tracing is used for shading, the second step of the rendering process, but having to implement both systems in the same applications requires more work than just using one unified framework. And since ray tracing makes it easier to simulate things such as reflections, then most people prefer to use ray tracing to solve the visibility problem as well.</p>
<h2 id="a-light-simulator">A Light Simulator</h2>
<p>We finished the last chapter on the idea that ray-tracing was better than rasterization to simulate important and common shading and lighting effects (such as reflections, soft shadows, etc.). Not being able to simulate these effects, simply means your image will lack the photo-realism we strive for. But before we dive into this topic further, let&rsquo;s have a look at some images from the real world to better understand what these effects are.</p>
<h3 id="reflection">Reflection</h3>
<p><a href="#R-image-efcfb1a3a55d05fa70c7ad5571a2cc43" class="lightbox-link"><img src="../assets/reflection.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-efcfb1a3a55d05fa70c7ad5571a2cc43"><img src="../assets/reflection.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p>When light comes in contact with a perfect mirror-like surface, it is reflected into the environment in a predictable direction. This new direction can be computed using the <strong>law of reflection</strong>. This law states that, like a tennis ball bouncing off the floor, a light ray changes direction when it comes in contact with a surface, and that the outgoing or <strong>reflected direction</strong> of this ray is a reflection of the incoming or <strong>incident direction</strong> about the normal at the point of incidence. A more formal way of defining the law of reflection is to say that a reflected ray always comes off the surface of a material at an angle equal to the angle at which the incoming ray hit the surface. This is illustrated in the image on the right, where you can see that the angle between the normal and the incident vector, is equal to the angle between the normal and the outgoing vector. Note that even though we used a water surface in the picture as an example of a reflective surface, water and glass are pretty poor mirrors compared to metals particularly.</p>
<h3 id="transparency">Transparency</h3>
<p><a href="#R-image-d9deb05d8d8548761ac36fa884689a7f" class="lightbox-link"><img src="../assets/transparency.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-d9deb05d8d8548761ac36fa884689a7f"><img src="../assets/transparency.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><a href="#R-image-977d895505e1e370bd3a924ce6f9c34e" class="lightbox-link"><img src="../assets/transparent-raygraph.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-977d895505e1e370bd3a924ce6f9c34e"><img src="../assets/transparent-raygraph.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p>In the case of transparent objects (imagine a pane of glass for example), light is reflected and refracted. The term &ldquo;transmitted&rdquo; is also often used in place of &ldquo;refracted&rdquo;, but the terms mean two slightly different things. By transmitted we mean, a fraction of the incident light enters the object on one side and leaves the object on the other side (which is why we see objects through a window). However, as soon as it comes in contact with the surface of a transparent object, light changes direction, and this is what we call refraction. It is the effect of light rays being bent as they travel from one transparent medium such as air to another such as water or glass (it doesn&rsquo;t matter if it goes from air to water or water to air, light rays are still being bent in one way or another). As with reflection, the refraction direction can be computed using <strong>Snell&rsquo;s law</strong>. The amount of light reflected and refracted is given by the <strong>Fresnel&rsquo;s equation</strong>. These two equations are very important in rendering. The graph on the right, shows a primary ray going through a block of glass. The ray is refracted, then travels through the glass, is refracted again when it leaves the glass, and eventually hits the surface below it. If that surface was an object, then this is what we would see through the glass.</p>
<h3 id="glossy-or-specular-reflection">Glossy or Specular Reflection</h3>
<p><a href="#R-image-b9ea85dc15dfa5d8214b0f6f009c43c2" class="lightbox-link"><img src="../assets/glossyreflection.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-b9ea85dc15dfa5d8214b0f6f009c43c2"><img src="../assets/glossyreflection.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><a href="#R-image-d26d181efec9ad570c54d022a817f350" class="lightbox-link"><img src="../assets/specular-raygraph.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-d26d181efec9ad570c54d022a817f350"><img src="../assets/specular-raygraph.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p>A glossy reflection is a material that is not perfectly reflective (like a mirror) nor perfectly diffuse. It is somewhere in between, where this &ldquo;in-between&rdquo; can either be anywhere between almost perfectly reflective (as in the case of a mirror-like surface) and almost perfectly diffuse. The glossiness of a surface is also sometimes referred to as its <strong>roughness</strong> (the two terms are antonymous) and <strong>specular reflection</strong> is often used instead of glossy reflection. You will often come across these two terms in computer graphics. Why do we speak of roughness then? To behave like a mirror, the surface of an object needs to be perfectly smooth. While many objects may appear flat and smooth in appearance (with the naked eye), looking at their surface under a microscope, reveals a very complex structure, which is not flat or smooth at all. In computer graphics we often like to describe rough surfaces, using the image of a surface made of lots of microfacets, where each one of these microfacets is oriented in a slightly different direction and acts on its own as a perfect mirror. As you can see in the adjacent image, when light bounces off from one of these facets, it is reflected in a slightly different direction than the mirror direction. The amount of variation between the mirror direction and the ray outgoing direction depends on how strongly the facets deviate from an ideally smooth surface. The stronger the deviation, the greater the difference, on average, between the ideal reflection direction and the actual reflection direction. Visually, rather than having a perfect image of the environment reflected off of a mirror-like surface, this image is slightly deformed (or blurred if you prefer). We have all seen how ripples caused by a pebble thrown into the water, change the sharp image reflected by a perfectly still water surface. Glossy reflections are similar to that: a perfect reflection deformed or blurred by the microscopic irregularities of the surface.</p>
<p>In computer graphics, we often speak of <strong>scattering</strong>. Because rather than being all reflected in the same direction, rays are scattered in a range of directions around the mirror direction (as shown in the last image on the right).</p>
<h3 id="diffuse-reflection">Diffuse Reflection</h3>
<p><a href="#R-image-c95962ed1de4c8a3986665bed613d41c" class="lightbox-link"><img src="../assets/diffusereflection.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-c95962ed1de4c8a3986665bed613d41c"><img src="../assets/diffusereflection.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p>On the other extreme of a perfectly reflective surface, is the concept of diffuse surfaces. When we talked about specular reflection, we mentioned that light rays were scattered around the mirror direction. But for diffuse surfaces, rays are scattered even more, in fact so much, that they are reflected in all sorts of random directions. Incident light is equally spread in every direction above the point of incidence and as a result, a diffuse surface appears equally bright from all viewing directions (again that&rsquo;s because the incident light is equally spread in every direction as a result of being strongly scattered). Two things can cause a surface to be diffuse: the surface can either be very rough or made up of small structures (such as crystals). In the latter case, rays get trapped in these structures and are reflected and refracted by them a great number of times before they leave the surface. Each reflection or refraction with one of these structures changes the light direction, and it happens so many times that when they finally leave the surface, rays have a random direction. What we mean by random is that the outgoing direction does not correlate whatsoever with the incident direction. Or to put it differently, the direction of incidence does not affect the light rays&rsquo; outing directions (which is not the case of specular surfaces), which is another interesting property of diffuse surfaces.</p>
<h3 id="subsurface-scattering">Subsurface Scattering</h3>
<p><a href="#R-image-f3c872a69e236a96a9b056f0d9e229c0" class="lightbox-link"><img src="../assets/subsurface.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-f3c872a69e236a96a9b056f0d9e229c0"><img src="../assets/subsurface.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p>Subsurface scattering is the technical term for translucency. Translucent surfaces in a way are surfaces that are not completely opaque nor completely transparent. But in fact, the reason why objects are translucent has little to do with transparency. The effect is visible when wax, a small object made out of jade or marble, or when a thin layer of organic material (skin, leaves) is strongly illuminated from the back. Translucency is the effect of light traveling through the material, changing directions along it is way until and leaving the object in a different location and a different direction than the point and direction of incidence. Subsurface scattering is rather complex to simulate.</p>
<h3 id="indirect-diffuse">Indirect Diffuse</h3>
<p><a href="#R-image-30f34ca3a3879e9a4a5abfc2e961f312" class="lightbox-link"><img src="../assets/indirectdiffuse.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-30f34ca3a3879e9a4a5abfc2e961f312"><img src="../assets/indirectdiffuse.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p>Some surfaces of the ornamental object in the center of the adjacent image, are not facing any direct light at all. They are not facing the sun and they are not facing up to the sky either (which we can look at as a very large light source). And yet, they are not completely black. How come? This happens because the floor which is directly illuminated by the sun, bounces light back into the environment and some of that light eventually illuminates parts of the object which are not receiving any direct light from the sun. Because the surface receives light emitted by light sources such as the sun indirectly (through other surfaces), we speak of indirect lighting.</p>
<h3 id="indirect-specular-or-caustics">Indirect Specular or Caustics</h3>
<p><a href="#R-image-19579e45a22fa88aec1d2f9d4f260b7b" class="lightbox-link"><img src="../assets/caustics.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-19579e45a22fa88aec1d2f9d4f260b7b"><img src="../assets/caustics.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p>Similarly to the way diffuse objects reflect light that illuminates other objects in their surroundings, reflective objects too can indirectly illuminate other objects by redirecting light to other parts of their environment. Lenses or waves at the surface of the water also focus light rays within singular lines or patterns which we call caustics (we are familiar with the dancing pattern of light at the bottom of a pool exposed to sunlight). Caustics are also frequently seen when light is reflected off of the mirrors making up the surface of disco balls, reflected off of the surface of windows in summer, or when a strong light shines upon a glass object.</p>
<h3 id="soft-shadows">Soft Shadows</h3>
<p><a href="#R-image-b62aad1a2939ed8af4b224128dec3e7a" class="lightbox-link"><img src="../assets/softshadows.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-b62aad1a2939ed8af4b224128dec3e7a"><img src="../assets/softshadows.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p>Most of the effects we described so far have something to do with the object&rsquo;s material properties. Soft shadows on the other hand have nothing to do with materials. Simulating them is only a geometric problem involving the objects and light sources&rsquo; shape, size, and location in space.</p>
<p>Don&rsquo;t worry if you are curious about knowing and understanding how all these effects can be simulated. We will study them all in due time. At this point of the lesson, it&rsquo;s only important to look at some images of the real world, and analyze what lighting/shading effects we can observe in these images so that we can reproduce them later on.</p>
<blockquote>
<p>Remember from this chapter, that a diffuse surface appears equally bright from all viewing directions, but a specular surface&rsquo;s brightness varies with the viewing direction (if you move around a mirror, the image you see in the mirror will change). We say that diffuse interaction is <strong>view-independent</strong> while specular interaction is <strong>view-dependent</strong>.</p>
</blockquote>
<h3 id="light-transport-and-shading-two-related-but-different-problems">Light Transport and Shading: Two Related But Different Problems</h3>
<p>The other reason why we have been quickly reviewing these effects is for you to realize two things:</p>
<ul>
<li>
<p>The appearance of objects, only depends on the way light interacts with matter and travels through space.</p>
</li>
<li>
<p>All these effects can be broadly divided into two categories:</p>
<ul>
<li>Some effects relate to the way objects appear.</li>
<li>Some effects relate to how much light an object receives.</li>
</ul>
<p>In the former category, you can add reflection, transparency, specular reflection, diffuse reflection, and subsurface scattering. In the latter, you can add indirect diffuse, indirect specular, and soft shadows. The first category could relate to what we call <strong>shading</strong> (what gives an object its appearance), while the second can relate to what we call <strong>light transport</strong> (how is light transported from the surface to surface as a result of interacting with different materials).</p>
<p>In shading, we study the way light interacts with matter (or the other way around). In other words, it looks at everything that happens to light from the moment it reaches an object, to the moment it leaves it.</p>
<p>Light transport is the study of what happens to light when it bounces from surface to surface. How is it reflected from various surfaces? How does the nature of this reflection change with the type of material light is reflected from (diffuse, specular, etc.)? Where does light go? Is it blocked by any geometry on its way to another surface? What effect does the shape of that blocker have on the amount of light an object receives? More generally, light transport is interested in the <strong>paths</strong> light rays are to follow as they travel from a light source to the eye (which we call flight paths).</p>
</li>
</ul>
<p>Note that the boundary between shading and light transport is very thin. In the real world, there would be no distinction to be made. It&rsquo;s all about light traveling and taking different paths depending on the object it encounters along its way from the light source to the eye. But, it is convenient in computer graphics to make the distinction between the two because they can&rsquo;t be simulated efficiently using the same approach. Let&rsquo;s explain.</p>
<p>If we could replicate the world in our computer program down to the atom, and code some basic rules to define the way light interacts with these atoms, we would just have to wait for light to bounce around until it reaches our eye, to generate a perfect image of the world. Creating such a program would be ideal but unfortunately, it can&rsquo;t be done with our current technology. Even if you had enough memory to model the world at the atomic level, you&rsquo;d not have enough computing power to simulate the path of the zillions of light particles (photons) traveling around us and interacting a zillions times with matter almost instantaneously before it reaches the eye, in anything less than an infinite amount of time. Therefore, a different approach is required. What we do instead is look at what takes the most time in the process. Well clearly, light traveling in straight paths from one surface to another is pretty basic, while what happens when light reaches a surface and interacts with it, is complex (and is what would take the most time to simulate).</p>
<p>Thus, in computer graphics, we artificially make a distinction between shading and light transport. The art of shading is to design mathematical models that approximate the way light interacts with matter, at a fraction of the time it would take if these interactions were to be physically simulated. On the other hand, we can afford to simulate the path of light rays as they go from one surface to another, as nothing complex happens to them on their way. This distinction allows designing strategies adapted to solving both problems (shading and light transport) independently.</p>
<p>Simulating light transport is easier than simulating the interaction of light with matter, though, we didn&rsquo;t say it was easy. Some types of inter-reflection are notably hard to simulate (caustics, for instance, we will explain why in the next chapter), and while designing good mathematical models to emulate the way light interacts with surfaces is hard, designing a good light transport algorithm can be challenging on its own (as we will see in the next chapter).</p>
<h3 id="global-illumination">Global illumination</h3>
<p>But let&rsquo;s step back a little. While you may think (it&rsquo;s often a misconception) that most surfaces are visible because they receive light directly from a light source, there are about as many situations (if not many more) in which, light only appears visible as a result of being illuminated indirectly by other surfaces. Look around you and just compare the number of objects or roughly the ratio between the areas which are directly exposed to a light source (the sun, artificial lights, etc.), over areas that are not exposed directly to a light source and receive light reflected by another surface. Indirect lighting plays such an important part in the world as we see it, that if you don&rsquo;t simulate it, it will be hard to make your images look photo-real. When in rendering we can simulate both direct lighting and indirect lighting effects, we speak of <strong>global illumination</strong>. Ideally, in lighting, and rendering more generally, we want to simulate every possible lighting scenario. A scenario is defined by the shape of the object contained in the scene, its material, how many lights are in the scene, their type (is it the sun, is it a light bulb, a flame), their shape, and finally how objects are scattered throughout space (which influences how light travels from surface to surface).</p>
<p>In CG, we make a distinction between direct and indirect lighting. If you don&rsquo;t simulate indirect lighting you can still see objects in the scene due to direct lighting, but if you don&rsquo;t simulate direct lighting, then obviously the image will be black (in the old days, direct lighting was also used to be called local illumination in contrast to global illumination which is the illumination of surfaces by other surfaces). But why wouldn&rsquo;t we simulate indirect lighting anyway?</p>
<p>Essentially because it&rsquo;s slow and/or not necessarily easy to do. As we will explain in detail in the next chapter, light can interact with many surfaces before it reaches the eye. If we consider ray tracing, for now, we also explained that what is the most expensive to compute in ray tracing is the ray-geometry intersection test. The more interactions between surfaces you have to simulate, the slower the render. With direct lighting, you only need to find the intersection between the primary or camera or eye rays (the rays traced from the camera) and the geometry in the scene, and then cast a ray from each one of these intersections to the lights in the scene (this ray is called a shadow ray). And this is the least we need to produce an image (we could ignore shadows, but shadows are a very important visual clue that helps us figure out where objects are in space, particularly in relation to each other. It also helps to recognize objects&rsquo; shapes, etc.). If we want to simulate indirect lighting, many more rays need to be cast into the scene to &ldquo;gather&rdquo; information about the amount of light that bounces off the surface of other objects in the scene. Simulating indirect lighting in addition to direct lighting requires not twice as many rays (if you compare that number with the number of rays used to simulate direct lighting), but <a href="http://en.wikipedia.org/wiki/Order_of_magnitude" target="_blank">orders of magnitude</a> more (to get a visually and accurate good result). And since the ray-object intersect test is expensive, as mentioned before, the more rays, the slower the render. To make things worse, note that when we compute indirect lighting we cast new rays from a point P in the scene to gather information about the amount of light reflected by other surfaces towards P. What&rsquo;s interesting is that this actually requires that we compute the amount of light arriving at these other surfaces as well, which means that for each one of the surfaces we need to compute the amount of light reflected towards P, and we also need to compute direct and indirect lighting, which means spawning even more rays. As you may have noticed, this effect is recursive. This is again why indirect lighting is a potentially very expensive effect to simulate. It is not making your render twice as long but many times longer.</p>
<p>Why is it difficult? It&rsquo;s pretty straightforward if you use ray tracing (but eventually expensive). Ray tracing as we will explain in the next paragraph is a pretty natural way of thinking and simulating the way light flows in the natural world. It&rsquo;s easy from a simulation point of view because it offers a simple way to &ldquo;gather&rdquo; information about light reflected off of surfaces in the scene. If your system supports the ability to compute the intersection of rays with geometry, then you can use it to either solve the visibility problem or simulate direct and indirect lighting. However, if you use rasterization, how do you gather that information? It&rsquo;s a common misbelief to think that you need ray-tracing to simulate indirect lighting, but this is not true. Many alternatives to ray tracing for simulating indirect lighting exist (point cloud-based, photon maps, virtual point lights, shadow maps, etc. <strong>Radiosity</strong> is another method to compute global illumination. It&rsquo;s not very much used anymore these days but was very popular in the 80s early 90s); these methods also have their advantages and can be in some situations, a good (if not better) alternative to ray tracing. However again, the &ldquo;easy&rdquo; way is to use ray tracing if your system supports it.</p>
<p>As mentioned before, ray tracing can be slow compared to some other methods when it comes to simulating indirect lighting effects. Furthermore, while ray tracing is appealing in many ways, it also has its own set of issues (besides being computationally expensive). Noise for example is one of them. Interestingly, some of the alternative methods to ray tracing we talked about simulate indirect lighting and produce noise-free images (often at the expense of being biased though - we will explain what that term means in the lesson on Monte Carlo ray tracing but in short, it means that mathematically we know that the solution computed by these algorithms doesn&rsquo;t converge to the true solution (as it should), which is not the case with Monte Carlo ray tracing.</p>
<p>Furthermore, we will show in the next chapter devoted to light transport that some lighting effects are very difficult to simulate because, while it&rsquo;s more efficient in rendering to simulate the path of light from the eye back to light sources, in some specific cases, it happens that this approach is not efficient at all. We will show what these cases are in the next chapter, but within the context of the problem at hand here, it means that naive &ldquo;backward&rdquo; ray tracing is just not the solution to everything: while being efficient at simulating direct lighting and indirect diffuse effects, it is not a very efficient way of simulating other specific lighting effects such as indirect specular reflections (we will show why in the next chapter). In other words, unless you decide that brute force is okay (you generally do until you realize it&rsquo;s not practical to work with), you will quickly realize that &ldquo;naive&rdquo; backward ray tracing is clearly not the solution to everything, and potentially look for alternative methods. Photon maps are a good example of a technique designed to efficiently simulate caustics (a mirror reflecting light onto a diffuse surface for example — which is a form of indirect specular reflection) which are very hard or computationally expensive to simulate with ray tracing.</p>
<h3 id="why-is-ray-tracing-better-than-rasterization-is-it-better">Why is ray-tracing better than rasterization? Is it better?</h3>
<p>We already provided some information about this question in the previous paragraph. Again, ray-tracing is a more natural way of simulating how light flows in the real world so in a way, yes it&rsquo;s simply the most natural, and straightforward approach to simulating lighting, especially compared to other methods such as rasterization. And rather than dealing with several methods to solve the visibility problem and lighting, ray tracing can be used for both, which is another great advantage. All you need to do in a way is come up with the most possible efficient way of computing the intersection of rays with geometry, and keep re-using that code to compute whatever you need, whether visibility or lighting. Simple, easy. if you use rasterization for the visibility, you will need another method to compute global illumination. So while it&rsquo;s not impossible to compute GI (global illumination) if you don&rsquo;t use ray tracing, not doing so though requires a mismatch of techniques which is clearly less elegant (and primarily why people prefer to use ray tracing only).</p>
<p>Now, as suggested, ray tracing is not a miraculous solution though. It comes with its own set of issues. A naive implementation of ray tracing is simple. One that is efficient, requires a lot of hard work. Ray tracing is still computationally expensive and even if computers today are far more powerful than ten years ago, the complexity of the scene we render has also dramatically increased, and render times are still typically very long (see the note below).</p>
<blockquote>
<details>
This is called Blinn's Law or the paradox of increasing performance. "What is Blinn's Law? Most of you are familiar with Moore's law which states that the number of transistors on a chip will double approximately every two years. This means that anyone using a computer will have access to increased performance at a predictable rate. For computer graphics, potential benefits relative to increasing computational power are accounted for with this concept. The basic idea behind Blinn's law is that if an animation studio invests ten hours of computation time per frame of animation today, they will invest ten hours per frame ten years from now, regardless of any advances in processing power." ([courtesy of www.boxtech.com](http://boxxblogs.blogspot.co.uk)).
<p><a href="#R-image-4ad52e9c3ff9ead0f5c2ab797df8d006" class="lightbox-link"><img src="../assets/blinnlaw.jpg" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-4ad52e9c3ff9ead0f5c2ab797df8d006"><img src="../assets/blinnlaw.jpg" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
</details>
</blockquote>
<p>So you still need to aggressively optimize your code, to make it practical to work with (especially if you use it in a production environment). But if you put the technical problems aside for a moment, the main drawback of ray tracing is the noise (the technical term is variance) it introduces in the image and the difficulty of simulating some lighting effects such as caustics when you use backward ray tracing (tracing the rays back from the eye to the source). One way of solving both issues is brute force: simply use more rays to improve the quality of the simulation, however, the more rays you use the more expensive the image. Thus again, a lot of research in rendering went (and still goes) into finding solutions to these two particular problems. Light transport algorithms as we will explain in the next chapter, are algorithms exploring the different ways in which light transport can be simulated. And as we will see, ray tracing can also be combined with some other techniques to make it more efficient to simulate some lighting effects which are very hard (as in very expensive) to simulate with ray tracing alone.</p>
<p>To conclude, there&rsquo;s very little doubt though, that, all rendering solutions will ultimately migrate to ray-tracing at some point or another, including real-time technology and video games. It is just a matter of time. The most recent generation of GPUs supports hardware accelerated ray-tracing already (e.g. RTX) with real-time or near real-time (interactive) framerate. The framerate still depends on scene complexity (number of triangles/quads, number of lights, etc.)</p>
<h2 id="light-transport-1">Light Transport</h2>
<blockquote>
<p>It&rsquo;s neither simple nor complicated, but it is often misunderstood.</p>
</blockquote>
<h3 id="light-transport-2">Light Transport</h3>
<p>In a typical scene, light is likely to bounce off of the surface of many objects before it reaches the eye. As explained in the <a href="../../../../../lessons/3d-basic-rendering/rendering-3d-scene-overview/light-simulator">previous chapter</a>, the direction in which light is reflected depends on the material type (is it diffuse, specular, etc.), thus light paths are defined by all the successive materials the light rays interact with on their way to the eye.</p>
<p><a href="#R-image-2bbcc80f346ce80966d7aa5f630390d9" class="lightbox-link"><img src="../assets/lightpath.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-2bbcc80f346ce80966d7aa5f630390d9"><img src="../assets/lightpath.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 1:</strong> <em>light paths.</em></p>
<p>Imagine a light ray emitted from a light source, reflected off of a diffuse surface, then a mirror surface, then a diffuse surface again and then reaching the eye. If we label, the light L, the diffuse surface D, the specular surface S (a mirror reflection can be seen as an ideal specular reflection, one in which the roughness of the surface is 0) and the eye E, the light path in this particular example is LDSDE. Of course, you can imagine all sorts of possible combinations; this path can even be an &ldquo;infinitely&rdquo; long string of Ds and Ss. The one thing that all these rays will have in common, is an L at the start and an E at the end. The shortest possible light path is LE (you look directly at something that emits light). If light rays bounce off the surface only once, which using the light path notation could be expressed as either LSE or LDE, then we have a case of direct lighting (direct specular or direct diffuse). Direct specular is what you have when the sun is reflected off of a water surface for instance. If you look at the reflection of a mountain in the lake, you are more likely to have an LDSE path (assuming the mountain is a diffuse surface), etc. In this case, we speak of indirect lighting.</p>
<p>Researcher Paul Heckbert introduced the concept of labeling paths that way in a paper published in 1990 and entitled &ldquo;Adaptive Radiosity Textures for Bidirectional Ray Tracing&rdquo;. It is not uncommon to use regular expressions to describe light paths compactly. For example, any combination of reflection off the surface of a diffuse or specular surface can be written as: L(D|S)<em>E. In <a href="http://en.wikipedia.org/wiki/Regular_expression" target="_blank">Regex</a> (the abbreviation for regular expression), (a|b)</em> denotes the set of all strings with no symbols other than &ldquo;a&rdquo; and &ldquo;b&rdquo;, including the empty string: {&quot;&quot;, &ldquo;a&rdquo;, &ldquo;b&rdquo;, &ldquo;aa&rdquo;, &ldquo;ab&rdquo;, &ldquo;ba&rdquo;, &ldquo;bb&rdquo;, &ldquo;aaa&rdquo;, &hellip;}.</p>
<p><a href="#R-image-fba8cfc9ae1b9d50504e50a28f44fb1f" class="lightbox-link"><img src="../assets/shadow2.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-fba8cfc9ae1b9d50504e50a28f44fb1f"><img src="../assets/shadow2.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 2:</strong> <em>to compute direct lighting, we just need to cast a shadow ray from P to the light source. If the ray is blocked by an object on its way to the light, then P is in shadow.</em></p>
<p><a href="#R-image-b456cf455416900f52d3719dd2324b90" class="lightbox-link"><img src="../assets/indirect-lighting.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-b456cf455416900f52d3719dd2324b90"><img src="../assets/indirect-lighting.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 3:</strong> <em>to compute indirect lighting, we need to spawn secondary rays from P and check if these rays intersect other surfaces in the scene. If they do, we need to compute both indirect and direct lighting at these intersection points and return the amount of computed light to P. Note that this is a recursive process: each time a secondary ray hits a surface we need to compute both direct lighting and indirect lighting at the intersection point on this surface, which means spawning more secondary rays, etc.</em></p>
<p>At this point, you may think, &ldquo;this is all good, but how does that relate to rendering?&rdquo;. As mentioned several times already in this lesson and the previous one, in the real world, light goes from light sources to the eye. But only a fraction of the rays emitted by light sources reaches the eye. Therefore, rather than simulating light path from the source to the eye, a more efficient approach is to start from the eye, and walk back to the source.</p>
<p>This is what we typically do in ray tracing. We trace a ray from the eye (we generally call the <strong>eye ray</strong>, <strong>primary ray</strong>, or <strong>camera ray</strong>) and check whether this ray intersects any geometry in the scene. If it does (let&rsquo;s call P, the point where the ray intersects the surface), we then need to do two things: compute how much light arrives at P from the light sources (direct lighting), and how much light arrives at P indirectly, as a result of light being reflected by other surfaces in the scene (indirect lighting).</p>
<ul>
<li>To compute the direct contribution of light to the illumination of P, we trace a ray from P to the source. If this ray intersects another object on its way to the light, then P is in the shadow of this light (which is why we sometimes call these rays <strong>shadow rays</strong>). This is illustrated in figure 2.</li>
<li>Indirect lighting comes from other objects in the scene reflecting light towards P, whether as a result of these objects reflecting light from a light source or as a result of these objects reflecting light which is itself bouncing off of the surface of other objects in the scene. In ray tracing, indirect illumination is computed by spawning new rays, called <strong>secondary rays</strong> from P into the scene (figure 3). Let&rsquo;s explain in more detail how and why this works.</li>
</ul>
<p>If these secondary rays intersect other objects or surfaces in the scene, then it is reasonable to assume, that light travels along these rays from the surfaces they intersect to P. We know that the amount of light reflected by a surface depends on the amount of light arriving on the surface as well as the viewing direction. Thus to know how much light is reflected towards P along any of these secondary rays, we need to:</p>
<ul>
<li>Compute the amount of light arriving at the point of intersection between the secondary ray and the surface.</li>
<li>Measure how much of that light is reflected by that surface to P, using the secondary ray direction as our viewing direction.</li>
</ul>
<blockquote>
<details>
Remember that specular reflection is view-dependent: how much light is reflected by a specular surface depends on the direction from which you are looking at the reflection. Diffuse reflection though is view-independent: the amount of light reflected by a diffuse surface doesn't change with direction. Thus unless diffuse, a surface doesn't reflect light equally in all directions.
</details>
</blockquote>
<p>Computing how much light arrives at a point of intersection between a secondary ray and a surface, is no different than computing how much light arrives at P. Computing how much light is reflected in the ray direction towards P, depends on the surface properties, and is generally done in what we call a <strong>shader</strong>. We will talk about shaders in the next chapter.</p>
<blockquote>
<details>
Other surfaces in the scene potentially reflect light to P. We don't know which one and light can come from all possible directions above the surface at P (light can also come from underneath the surface if the object is transparent or translucent -- but we will ignore this case for now). However, because we can't test every single possible direction (it would take too long) we will only test a few directions instead. The principle is the same as when you want to measure for instance the average height of the adult population of a given country. There might be too many people in this population to compute that number exactly, however, you can take a sample of that population, let's say maybe a few hundreds or thousands of individuals, measure their height, make an average (sum up all the numbers and divide by the size of your sample), and get that way, an approximation of the actual average adult height of the entire population. It's only an approximation, but hopefully, it should be close enough to the real number (the bigger the sample, the closer the approximation to the exact solution). We do the same thing in rendering. We only sample a few directions and assume that their average result, is a good approximation of the actual solution. If you heard about the term **Monte Carlo** before and particularly **Monte Carlo ray tracing**, that's what this technique is all about. Shooting a few rays to approximate the exact amount of light arriving at a point. The downside is that the result is only an approximation. The bright side is that we get a result for a problem that is otherwise not tractable (e.i. it is impossible to compute exactly within any amount of reasonable finite time).
</details>
</blockquote>
<p>Computing indirect illumination is a <strong>recursive</strong> process. Secondary rays are generated from P, which in turn generate new intersection points, from which other secondary rays are generated, and so on. We can count the number of times light is reflected from surfaces from the light source until it reaches P. If light bounces off the surface of objects only once before it gets to P we have&hellip; one bounce of indirect illumination. Two bounces, light bounces off twice, three bounces, three times, etc.</p>
<p><a href="#R-image-d03390289e3630b50235ed0929063905" class="lightbox-link"><img src="../assets/indirect-bounce.gif" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-d03390289e3630b50235ed0929063905"><img src="../assets/indirect-bounce.gif" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 4:</strong> <em>computing indirect lighting is a recursive process. Each time a secondary ray hits a surface, new rays are spawned to compute indirect lighting at the intersection point.</em></p>
<p>The number of times light bounces off the surface of objects can be infinite (imagine a situation for example in which a camera is inside a box illuminated by a light on the ceiling? rays would keep bouncing off the walls forever). To avoid this situation, we generally stop spawning secondary rays after a certain number of bounces (typically 1, 2, or 3). Note though that as a result of setting a limit to the number of bounces, P is likely to look darker than it actually should (since any fraction of the total amount of light emitted by a light source that took more bounces than the limit to arrive at P, will be ignored). If we set the limit to two bounces for instance, then we ignore the contribution of all the other bounces above (third, fourth, etc.). However luckily enough, each time light bounces off of the surface of an object, it loses a little bit of its energy. This means that as the number of bounces increases, the contribution of these bounces to the indirect illumination of a point decreases. Thus, there is a point after which you might consider that computing one more bounce makes such a little difference to the image, that it doesn&rsquo;t justify the amount of time it takes to simulate it.</p>
<blockquote>
<details>
If we decide, for example, to spawn 32 rays each time we intersect a surface to compute the amount of indirect lighting (and assuming each one of these rays intersects a surface in the scene), then on our first bounce we have 32 secondary rays. Each one of these secondary rays generates another 32 secondary rays. Which makes already a total of 1024 rays. After three bounces we generated a total of 32768 rays! If ray tracing is used to compute indirect lighting, it generally becomes quickly very expensive because the number of rays grows exponentially as the number of bounces increases. This is often referred to as the curse of ray tracing.
</details>
</blockquote>
<p><a href="#R-image-ccde4bb1dd55e5b69808a99247c73565" class="lightbox-link"><img src="../assets/shadow.gif" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-ccde4bb1dd55e5b69808a99247c73565"><img src="../assets/shadow.gif" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p>Figure 5: when we compute direct lighting, we need to cast a shadow ray from the point where the primary ray intersected geometry to each light source in the scene. If this shadow ray intersects another object &ldquo;on its way to the light source&rdquo;, then this point is in shadow.</p>
<p>This long explanation is to show you, that the principle of actually computing the amount of light impinging upon P whether directly or indirectly is simple, especially if we use the ray-tracing approach. The only sacrifice to physical accuracy we made so far, is to put a cap on the maximum number of bounces we compute, which is necessary to ensure that the simulation will not run forever. In computer graphics, this algorithm is known as <strong>unidirectional path tracing</strong> (it belongs to a larger category of light transport algorithms known as path tracing). This is the simplest and most basic of all <strong>light transport models</strong> based on ray tracing (it also goes by the name of classic ray tracing or Whitted style ray tracing). It&rsquo;s called unidirectional, because it only goes in one direction, from the eye to the light source. The part &ldquo;path tracing&rdquo; is pretty straightforward: it&rsquo;s all about tracing light paths through the scene.</p>
<blockquote>
<p>Classic ray tracing generates a picture by tracing rays from the eye into the scene, recursively exploring specularly reflected and transmitted directions, and tracing rays toward point light sources to simulate shadows. (Paul S. Heckbert - 1990 in &ldquo;Adaptive Radiosity Textures for Bidirectional Ray Tracing&rdquo;)</p>
</blockquote>
<p>This method was originally proposed by <strong>Appel</strong> in 1986 (&ldquo;Some Techniques for Shading Machine Rendering of Solids&rdquo;) and later developed by <strong>Whitted</strong> (An improved illumination model for shaded display - 1979).</p>
<blockquote>
<details>
When the algorithm was first developed, Appel and Whitted only considered the case of mirror surfaces and transparent objects. This is only because computing secondary rays (indirect lighting) for these materials require fewer rays than for diffuse surfaces. To compute the indirect reflection of a mirror surface, you only need to cast one single reflection ray into the scene. If the object is transparent, you need to cast one ray for the reflection and one ray for the refraction. However, when the surface is diffuse, to approximate the amount of indirect lighting at P, you need to cast many more rays (typically 16, 32, 64, 128 up to 1024 - this number though doesn't have a power of 2 but it usually is for reasons will explain in due time) distributed over the hemisphere oriented about the normal at the point of incidence. This is far more costly than just computing reflection and refraction (either one or two rays per shaded point), so their first developed their concept by using specular and transparent surfaces to start with as computers back then were very slow compared to today's standards; but extending their algorithm to indirect diffuse was, of course, straightforward.
</details>
</blockquote>
<p>Other techniques than ray tracing can be used to compute global illumination. Note though that ray tracing seems to be the most adequate way of simulating the way light spreads out in the real world. But things are not that simple. With unidirectional path tracing, for example, some light paths are more complicated to compute efficiently than others. This is particularly true of light paths involving specular surfaces illuminating diffuse surfaces (or any type of surfaces for that matter) indirectly. Let&rsquo;s take an example.</p>
<p><a href="#R-image-c86c7f0999dac3ac4068f20d3b4c7217" class="lightbox-link"><img src="../assets/caustics2.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-c86c7f0999dac3ac4068f20d3b4c7217"><img src="../assets/caustics2.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 6:</strong> <em>all light rays at point P come from the glass ball, but when secondary rays are spawned from P to compute indirect lighting, only a fraction of these rays will hit the ball. We fail to account for the fact that all light illuminating P is transmitted by the ball; the computation of the amount of indirect lighting arriving at P using backward tracing in this particular case, is likely to be quite inaccurate.</em></p>
<p>As you can see in the image above, in this particular situation, light emitted by the source at the top of the image, is refracted through a (transparent) glass ball which by the effect of refraction, concentrates all light rays towards a singular point on the plane underneath. This is what we call a caustic. Note that, no direct light arrives at P from the light source directly (P is in the &lsquo;shadow&rsquo; of the sphere). It all comes indirectly through the sphere by the mean of refraction and transmission. While it may seem more natural in this particular situation to trace light from the light source to the eye, considering that we decided to trace light rays the other way around, let&rsquo;s see what we get.</p>
<p>When it will come to computing how much light arrives at P indirectly if we assume that the surface at P is diffuse, then we will spawn a bunch of rays in random directions to check which surfaces in the scene redirect light towards P. But by doing so, we will fail to account for the fact that all light comes from the bottom surface of the sphere. So obviously we could maybe solve this problem by spawning all rays from P toward the sphere, but since our approach assumes we have no prior knowledge of how light travels from the light source to every single point in the scene, that&rsquo;s not something we can do (we have no prior knowledge that a light source is above the sphere and no reason to assume that this light is the light that contributes to the illumination of P via transmission and refraction). All we can do is spawn rays in random directions as we do with all other surfaces, which is how unidirectional path tracing works. One of these rays might actually hit the sphere and get traced back to the light source (but we don&rsquo;t even have a guarantee that even a single ray will hit the sphere since their directions are chosen randomly), however, this might only be one ray over maybe 10 or 20 or 100 we cast into the scene, thus we might miserably fail in this particular case to compute how much light arrives at P indirectly.</p>
<blockquote>
<details>
Isn't 1 ray over 10 or 20 enough? Yes and no. It's hard to explain the technique used here to "approximate" the indirect lighting component of the illumination of P but in short, it's based on probabilities and is very similar in a way to measuring an "approximation" of a given variable using a poll. For example, when you want to measure the average height of the adult population of a given country, you can't measure the height of every person making up that population. Instead, you just take a sample, a subset of that population, measure the average height of that sample and assume that this number is close enough to the actual average height of the entire population. While the theory behind this technique is not that simple (you need to prove that this approach is mathematically correct and not purely empirical), the concept is pretty simple to understand. We do the same thing here to approximate the indirect lighting component. We chose random directions, measure the amount of light coming from these directions, average the result, and assume the resulting number is an "approximation" of the actual amount of indirect light received by P. This technique is called Monte Carlo integration. It's a very important method in rendering and you will find it explained in great detail in a couple of lessons from the "Mathematics and Physics of Computer Graphics" section. If you want to understand why 1 ray over 20 secondary rays is not ideal in this particular case, you will need to read these lessons.
</details>
</blockquote>
<p>Using Heckbert light path&rsquo;s naming convention, we can say that paths of the kind LS+DE are generally hard to simulate in computer graphics using the basic approach of tracing back the path of light rays from the eye to the source (or unidirectional path tracing). In Regex, the + sign account for any sequences that match the element preceding the sign one or more times. For example, ab+c matches &ldquo;abc&rdquo;, &ldquo;abbc&rdquo;, &ldquo;abbbc&rdquo;, and so on, but not &ldquo;ac&rdquo;. What this means in our case, is that situations in which light is reflected off of the surface of one or more specular surfaces before it reaches a diffuse surface and then the eye (as in the example of the glass sphere), are hard to simulate using unidirectional path tracing.</p>
<p>What do we do then? This is where the art of light transport comes into play.</p>
<p>While being simple and thus very appealing for this reason, a naive implementation of tracing light paths to the eye is not efficient in some cases. It seems to work well when the scene is only made of diffuse surfaces but is problematic when the scene contains a mix of diffuse and specular surfaces (which is more often the case than not). So what do we do? Well, we do the same thing as we usually do when we have a problem. We search for a solution. And in this particular case, this leads to looking for developing strategies (or algorithms) that would work well to simulate all sorts of possible combinations of materials. We want a strategy in which LS+DE paths can be simulated as efficiently as LD+E paths. And since our default strategy doesn&rsquo;t work well in this case, we need to come up with new ones. This led obviously to the development of new <strong>light transport algorithms</strong> that are better than unidirectional path tracing to solve this light transport problem. More formally light transport algorithms are strategies (implemented in the form of algorithms) that attempt to propose a solution to the problem we just presented: solving efficiently any combination of any possible light path, or more generally light transport.</p>
<p>Light transport algorithms are not that many, but still, quite a few exist. And don&rsquo;t be misled. Nothing in the rules of coming up with the greatest light transport algorithm of all times, tells you that you have to use ray tracing to solve the problem. You have the choice of weapon. Many solutions use what we call a hybrid or multi-passes approach. <strong>Photon mapping</strong> is an example of such an algorithm. They require the pre-computation of some lighting information stored in specific data structures (a photon map or a point cloud generally for example), before actually rendering the final image. Difficult light paths are resolved more efficiently by taking advantage of the information stored in these structures. Remember that we said in the glass sphere example that we had no prior knowledge of the existence of the light above the sphere? Well, photon maps are a way of looking at the scene before it gets rendered and trying to get some prior knowledge about where light &ldquo;photons&rdquo; go before rendering the final image. It is based on that idea.</p>
<p>While being quite popular some years ago, these algorithms though are based on a multi-pass approach. In other words, you need to generate some extra data before you can render your final image. This is great if it helps to render images you couldn&rsquo;t render otherwise, but multi-passes rendering is a pain to manage, requires a lot of extra work, requires generally to store extra data on disk, and the process of actually rendering the image doesn&rsquo;t start before all the pre-computation steps are complete (thus you need to wait for a while before you can see something). As we said, for a long time they were popular because they made it possible to render things such as caustics which would have been too long to render with pure ray tracing, and that therefore, we generally ignored altogether. Thus having a technique to simulate them (no matter how painful it is to set up) is better than nothing. However, of course, a unified approach is better: one in which the multi-pass is not required and one which integrates smoothly with your existing framework. For example, if you use ray tracing (as your framework), wouldn&rsquo;t it be great to come up with an algorithm that only uses ray tracing, and never have to pre-compute anything? Well, it does exist.</p>
<p>Several algorithms have been developed around ray tracing and ray tracing only. Extending the concept of unidirectional path tracing, which we talked about above, we can use another algorithm known as bi-directional path tracing. It is based on the relatively simple idea, that for every ray you spawn from the eye into the scene, you can also spawn a ray from a light source into the scene, and then try to connect their respective paths through various strategies. An entire section of Scratchapixel is devoted to light transport and we will review in this section, some of the most important light transport algorithms, such as unidirectional path tracing, bi-directional path tracing, Metropolis light transport, instant radiosity, photon mapping, radiosity caching, etc.</p>
<h3 id="summary-3">Summary</h3>
<p>Probably one of the most common myths in computer graphics is that ray tracing is both the ultimate and only way to solve global illumination. While it may be the ultimate way in the sense that it offers a much more natural way of thinking about the way light travels in the real world, it also has its limitations, as we showed in this introduction, and it is certainly not the only way. You can broadly distinguish between two sorts of light transport algorithms:</p>
<ul>
<li>Those who are not using ray tracing such as photon or shadow mapping, radiosity, etc.</li>
<li>Those who are using ray tracing and ray tracing only.</li>
</ul>
<p>As long as the algorithm efficiently captures light paths that are difficult to capture with the traditional unidirectional path tracing algorithm, it can be viewed as one of the contendors to solve our LS+DE problem.</p>
<p>Modern implementations do tend to favor the light transport method solely based on ray tracing, simply because ray tracing is a more natural way to think about light propagation in a scene, and offers a unified approach to computing global illumination (one in which using auxiliary structures or systems to store light information is not necessary). Note though that while such algorithms do tend to be the norm these days in off-line rendering, real-time rendering systems are still very much based on the former approach (they are generally not designed to use ray tracing, and still rely on things such as shadow maps or light fields to compute direct and indirect illumination).</p>
<h2 id="shading-1">Shading</h2>
<p>While everything in the real world is the result of light interacting with matter, some of these interactions are too complex to simulate using the light transport approach. This is when shading kicks in.</p>
<p><a href="#R-image-f128230e088ef2696a248e3bc080efee" class="lightbox-link"><img src="../assets/color-brightness.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-f128230e088ef2696a248e3bc080efee"><img src="../assets/color-brightness.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 1:</strong> <em>if you look at two objects under the same lighting conditions if these objects seem to have the same color (same hue), but that one is darker than the other, then clearly, how bright they are is not the result of how much light falls on these objects, but more the result of how much light each one of these objects reflects into their environment.</em></p>
<p>As mentioned in the previous chapter, simulating the appearance of an object requires that we can compute the color and the brightness of each point on the surface of that object. Color and brightness are tightly linked with each other. You need to distinguish between the brightness of an object which is due to how much light falls on its surface, and the brightness of an object&rsquo;s color (also sometimes called the color&rsquo;s luminance). The brightness of color as well as its hue and saturation is a color property. If you look at two objects under the same lighting conditions, if these objects seem to have the same color (same <a href="http://en.wikipedia.org/wiki/Chromaticity" target="_blank">chromaticity</a>), but that one is darker than the other, then clearly, how bright they are is not the result of how much light falls on these objects, but more the result of how much light each one of these objects reflects into their environment. In other words, these two objects have the same color (the same chromaticity) but one reflects more light than the other (or to put it differently one absorbs more light than the other). The brightness (or luminance) of their color is different. In computer graphics, the characteristic color of an object is called <strong><a href="http://en.wikipedia.org/wiki/Albedo" target="_blank">albedo</a></strong>. The albedo of objects can be measured precisely.</p>
<blockquote>
<details>
Note that an object **can not** reflect more light than it receives (unless it emits light, which is the case of light sources). The color of an object can generally be computed (at least for diffuse surfaces) as the ratio of reflected light over the amount of incoming (white) light. Because an object can not reflect more light than it receives, this ratio is always lower than 1. This is why the colors of objects are always defined in the RGB system between 0 and 1 if you use float or 0 and 255 if you a byte to encode colors. Check the lesson on [Colors](/lessons/digital-imaging/colors/) to learn more about this topic. It's better to define this ratio as a percentage. For instance, if the ratio, the color, or the albedo (these different terms are interchangeable) is 0.18, then the object reflects 18% of the light it receives back in the environment.
</details>
</blockquote>
<p>If we defined the color of an object as the ratio of the amount of reflected light over the amount of light incident on the surface (as explained in the note above), that color can&rsquo;t be greater than one. This doesn&rsquo;t mean though that the amount of light incident and reflected off of the surface of an object can&rsquo;t be greater than one (it&rsquo;s only the ratio between the two that can&rsquo;t be greater than one). What we see with our eyes, is the amount of light incident on a surface, multiplied by the object&rsquo;s color. For example, if the energy of the light impinging upon the surface is 1000, and the color of the object is 0.5, then the amount of light reflected by the surface to the eye is 500 (this is wrong from the point of view of physics, but this is just for you to get the idea - in the lesson on shading and light transport, we will look into what this 1000 or 500 values mean in terms of physical units, and learn that it&rsquo;s more complicated than just multiplying the number of photons by 0.5 or whatever the albedo of the object is).</p>
<p>Thus assuming we know what the color of an object is, to compute the actual brightness of a point P on the surface of that object under some given lighting conditions (brightness as in the actual amount of light energy reflected by the surface to the eye and not as in the actual brightness or luminance of the object&rsquo;s albedo), we need to account for two things:</p>
<ul>
<li>How much light falls on the object at this point?</li>
<li>How much light is reflected at this point in the viewing direction?</li>
</ul>
<p>Remember again that for specular surfaces, the amount of light reflected by that surface depends on the angle of view. If you move around a mirror, the image you see in the mirror changes: the amount of light reflected towards you changes with the viewpoint.</p>
<p><a href="#R-image-e4ce691d2c24fbd6b49e985bd4aa9a57" class="lightbox-link"><img src="../assets/shading1.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-e4ce691d2c24fbd6b49e985bd4aa9a57"><img src="../assets/shading1.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 2:</strong> <em>to compute the actual brightness of a point P on the surface of that object under some given lighting conditions, we need to account for two things, how much light falls on the object at this point and how much light is reflected at this point in the viewing direction. To compute how much light arrives upon P, we need to sum up the contribution of the light sources (direct lighting) and from other surfaces (indirect lighting).</em></p>
<p>Assuming we know what the color of the object is (its albedo), we then need to find how much light arrives at the point (let&rsquo;s call it P again), and how much is reflected in the viewing direction, the direction from P to the eye.</p>
<ul>
<li>The former problem requires &ldquo;collecting&rdquo; or <strong>gathering</strong> light above the surface at P, and is more of a light transport problem. We already explained in the previous chapter how this can be done. Rays can be traced directly to lights to compute direct lighting and secondary rays can be spawned from P to compute indirect lighting (the contribution of other surfaces to the illumination of P). However, while it seems essentially like a light transport problem, we will see in the lessons on Shading and Light Transport that the direction of these rays is defined by the surface type (is it diffuse or specular), and that shaders play a role in choosing the direction of these rays. Note also that, other methods than ray tracing can be used to compute both direct and indirect lighting.</li>
<li>The latter problem (how much light is reflected in a given direction) is far more complex and it will now be explained in more detail.</li>
</ul>
<p>First, you need to remember that light reflected in the environment by a surface, is the result of very complex interactions between light rays (or photons if you know what they are) with the material the object is made of. There are three important things to note at this point:</p>
<ul>
<li>These interactions are generally so complex that it is not practical to simulate them.</li>
<li>The amount of light reflected depends on the view direction. Surfaces generally don&rsquo;t reflect incident light equally in all directions. That&rsquo;s not true of perfectly diffuse surfaces (diffuse surfaces appear equally bright from all viewing directions,) but this is true of all specular surfaces and since most objects in the real world have a mix of diffuse and specular reflections anyway, more often than not, light is generally not reflected equally.</li>
<li>The amount of light redirected in the viewing direction, also depends on the incoming light direction. To compute how much light is reflected in the direction (\omega_v) (&ldquo;v&rdquo; here is used here for the view and (\omega) is the Greek letter omega), we also need to take into account the incoming lighting direction (\omega_i) (&ldquo;i&rdquo; stands for incident or incoming). The idea is illustrated in figure 3. Let&rsquo;s see what happens when the surface from which a ray is reflected, is a mirror. According to the law of reflection, the angle between the incident direction (\omega_i) of a light ray and the normal at the point of incidence, and the direction between the reflected or mirror direction (\omega_r) and the normal are the same. When the viewing direction (\omega_v) and the reflected direction (\omega_r) are the same (figure 3 - top), then we see the reflected ray (it enters the eye). However when (\omega_v) and (\omega_r) are different (figure 3 - bottom), then because the reflected ray doesn&rsquo;t travel towards the eye, the eye doesn&rsquo;t see it. Thus, how much light is reflected towards the eye depends on the incident light direction (\omega_i) (as explained before) as well as the viewing direction (\omega_v).</li>
</ul>
<p><a href="#R-image-74e9b83de50bdcebe64ff97132866f31" class="lightbox-link"><img src="../assets/reflection2.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-74e9b83de50bdcebe64ff97132866f31"><img src="../assets/reflection2.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 3:</strong> <em>for mirror surfaces, if the reflected ray and the view direction are not the same, the reflected ray of light is not visible. The amount of light reflected is a function of the incoming light direction and the viewing direction.</em></p>
<blockquote>
<p><strong>Let&rsquo;s summarise</strong>. What do we know?</p>
<ul>
<li>It&rsquo;s too complex to simulate light-matter interactions (interactions happening at the microscopic and atomic levels). Thus, we need to come up with a different solution.</li>
<li>The amount of light reflected from a point varies with the view direction (\omega_v).</li>
<li>The amount of light reflected from a point for a given view direction (\omega_v), depends on the incoming light direction (\omega_i).</li>
</ul>
</blockquote>
<p><strong>Shading, which you can see as the part of the rendering process that is responsible for computing the amount of light reflected from surfaces to the eye (or other surfaces in the scene)</strong>, depends on at least two variables: where light comes from (the incident light direction (\omega_i)) and where it goes to (the outgoing or viewing direction, (\omega_v)). Where light comes from is independent of the surface itself, but how much light is reflected in a given direction, depends on the surface type: is it diffuse, or specular? As suggested before, gathering light arriving at the incident point is more of a light transport problem. But regardless of the technique used to gather the amount of light arriving at P, what we need, is to know where this light comes from, as in from which direction. The job of putting all these things together is done by what we call a <strong>shader</strong>. A shader can be seen as a program within your program, which is a kind of routine that takes an incident light direction and a view direction as input variables and returns the fraction of light the surface would reflect for these directions.</p>

<span class="math align-center">$$
\text{ ratio of reflected light = shader }(\omega_i, \omega_o)
$$</span><p>Simulating light-matter interactions to get a result is complex, but hopefully, the result of these numerous interactions is predictable and consistent, thus it can be approximated or modeled with mathematical functions. Where are these functions coming from? What are they? How are they found? We will answer these questions in the lessons devoted to shading. Let&rsquo;s only try to get an intuition of how and why this works for now.</p>
<p>The law of reflection for example which we introduced in a previous chapter, can be written as:</p>

<span class="math align-center">$$
\omega_r = \omega_i - 2(N \cdot \omega_i) N
$$</span><p>In plain English, it says that the reflection direction (\omega_r), can be computed as (\omega_i) minus two times the dot product between N (the surface normal at the point of incidence) and (\omega_i) (the incident light direction) multiplied by N. This equation has more to do with computing a direction than the amount of light reflected by the surface. However if for any given incident direction ((\omega_i)), you find out that (\omega_r) coincides with (\omega_v) (the view direction) then clearly, the ratio of reflected light for this particular configuration is 1 (figure 3 - top). If (\omega_r) and (\omega_v) are different though, then the amount of reflected light would be 0. To formalize this idea, you can write:</p>

<span class="math align-center">$$
\text {ratio of reflected light} = \begin{cases} 1 & \omega_r = \omega_o \\ 0 & \text{otherwise} \end{cases} 
$$</span><blockquote>
<details>
This is just an example. For perfectly mirror surfaces, we never proceed that way. The point here is to understand that if we can describe the behavior of light with equations, then we can find ways of computing how much light is reflected for any given set of incident and outgoing directions without having to run a complex and time-consuming simulation. This is really what shaders do: replacing complex light-matter interactions with a mathematical model, which is fast to compute. These models are not always very accurate nor physically plausible as we will soon see, but they are the most practical way of approximating the result of these interactions. Research in the field of shading is mostly about developing new mathematical models that match as closely as possible the way materials reflect light. As you may imagine this is a difficult task: it's challenging on its own, but more importantly, materials exhibit some very different behaviors thus it's generally impossible to simulate accurately all materials with one single model. Instead, it is often necessary to develop one model that works for example to simulate the appearance of cotton, one to simulate the appearance of silk, etc.
</details>
</blockquote>
<p>What about simulating the appearance of a diffuse surface? For diffuse surfaces, we know that light is reflected equally in all directions. The amount of light reflected towards the eye is thus the total amount of light arriving at the surface (at any given point) multiplied by the surface color (the <strong>fraction</strong> of the total amount of incident light the surface reflects in the environment), divided by some normalization factor that needs to be there for mathematical/physical accuracy reason, but this will be explained in details in the lessons devoted to shading. Note that for diffuse reflections, the incoming and outgoing light directions do not influence the amount of reflected light. But this is an exception in a way. For most materials, the amount of reflected light depends on (\omega_i) and (\omega_v).</p>
<p>The behavior of glossy surfaces is the most difficult to reproduce with equations. Many solutions have been proposed, the simplest (and easiest to implement in code) being the Phong specular model, which you may have heard about.</p>
<blockquote>
<details>
The Phong model computes the perfect mirror direction using the equation for the law of reflection which depends on the surface normal and the incident light direction. It then computes the deviation (or difference) raised to some exponent, between the actual view direction and the mirror direction (it takes the dot product between these two vectors) and assumes that the brightness of the surface at the point of incidence, is inversely proportional to this difference. The smaller the difference, the shinier the surface. The exponent parameter helps control the spread of the specular reflection (check the lessons from the Shading section to learn more about the Phong model).
</details>
</blockquote>
<p>However good models follow some well know properties that the Phong model doesn&rsquo;t have. One of these rules, for instance, is that the model <strong>conserves energy</strong>. The amount of light reflected in all directions shouldn&rsquo;t be greater than the total amount of incident light. If a model doesn&rsquo;t have this property (the Phong model doesn&rsquo;t have that property), then it would break the laws of physics, and while it might provide a visually pleasing result, it would not produce a physically plausible one.</p>
<blockquote>
<details>
Have you already heard the term **physically plausible rendering**? It designates a rendering system designed around the idea that all shaders and light transport models comply with the laws of physics. In the early age of computer graphics, speed and memory were more important than accuracy and a model was often considered to be good if it was fast and had a low memory footprint (at the expense of being accurate). But in our quest for photo-realism and because computers are now faster than they were when the first shading models were designed), we don't trade accuracy for speed anymore and use physically-based models wherever possible (even if they are slower than non-physically based models). The conservation of energy is one of the most important properties of a physically-based model. Existing physically based rendering engines can produce images of great realism.
</details>
</blockquote>
<p>Let&rsquo;s put these ideas together with some pseudo-code:</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="ln"> 1</span><span class="cl"><span class="n">Vec3f</span> <span class="nf">myShader</span><span class="p">(</span><span class="n">Vec3f</span> <span class="n">Wi</span><span class="p">,</span> <span class="n">Vec3f</span> <span class="n">Wo</span><span class="p">)</span> 
</span></span><span class="line"><span class="ln"> 2</span><span class="cl"><span class="p">{</span> 
</span></span><span class="line"><span class="ln"> 3</span><span class="cl">    <span class="c1">// define the object&#39;s color, roughness, etc.
</span></span></span><span class="line"><span class="ln"> 4</span><span class="cl"><span class="c1"></span>    <span class="p">...</span> 
</span></span><span class="line"><span class="ln"> 5</span><span class="cl">    <span class="c1">// do some mathematics to compute the ratio of light reflected
</span></span></span><span class="line"><span class="ln"> 6</span><span class="cl"><span class="c1"></span>    <span class="c1">// by the surface for this pair of directions (incident and outgoing)
</span></span></span><span class="line"><span class="ln"> 7</span><span class="cl"><span class="c1"></span>    <span class="p">...</span> 
</span></span><span class="line"><span class="ln"> 8</span><span class="cl">    <span class="k">return</span> <span class="n">ratio</span><span class="p">;</span> 
</span></span><span class="line"><span class="ln"> 9</span><span class="cl"><span class="p">}</span> 
</span></span><span class="line"><span class="ln">10</span><span class="cl"> 
</span></span><span class="line"><span class="ln">11</span><span class="cl"><span class="n">Vec3f</span> <span class="nf">shadeP</span><span class="p">(</span><span class="n">Vec3f</span> <span class="n">ViewDirection</span><span class="p">,</span> <span class="n">Vec3f</span> <span class="n">Point</span><span class="p">,</span> <span class="n">Vec3f</span> <span class="n">SurfaceNormal</span><span class="p">)</span> 
</span></span><span class="line"><span class="ln">12</span><span class="cl"><span class="p">{</span> 
</span></span><span class="line"><span class="ln">13</span><span class="cl">    <span class="n">Vec3f</span> <span class="n">totalAmountReflected</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> 
</span></span><span class="line"><span class="ln">14</span><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="n">all</span> <span class="n">light</span> <span class="n">sources</span> <span class="n">above</span> <span class="n">P</span> <span class="p">[</span><span class="n">direct</span><span class="o">|</span><span class="n">indirect</span><span class="p">])</span> <span class="p">{</span> 
</span></span><span class="line"><span class="ln">15</span><span class="cl">        <span class="n">totalAmountReflected</span> <span class="o">+=</span> 
</span></span><span class="line"><span class="ln">16</span><span class="cl">            <span class="n">lightEnergy</span> <span class="o">*</span> 
</span></span><span class="line"><span class="ln">17</span><span class="cl">            <span class="n">shaderDiffuse</span><span class="p">(</span><span class="n">LightDirection</span><span class="p">,</span> <span class="n">ViewDirection</span><span class="p">)</span> <span class="o">*</span> 
</span></span><span class="line"><span class="ln">18</span><span class="cl">            <span class="n">dotProduct</span><span class="p">(</span><span class="n">SurfaceNormal</span><span class="p">,</span> <span class="n">LightDirection</span><span class="p">);</span> 
</span></span><span class="line"><span class="ln">19</span><span class="cl">    <span class="p">}</span> 
</span></span><span class="line"><span class="ln">20</span><span class="cl"> 
</span></span><span class="line"><span class="ln">21</span><span class="cl">    <span class="k">return</span> <span class="n">totalAmountReflected</span><span class="p">;</span> 
</span></span><span class="line"><span class="ln">22</span><span class="cl"><span class="p">}</span> 
</span></span></code></pre></div><p>Notice how the code is separated into two main sections: a routine (line 11) to gather all light coming from all directions above P, and another routine (line 1), the shader, used to compute the fraction of light reflected by the surface for any given pair of incident and view direction. The loop (often called a light loop) is used, to sum up, the contribution of all possible light sources in the scene to the illumination of P. Each one of these light sources has a certain energy and of course, comes from a certain direction (the direction defined by the line between P and the light source position in space), thus all we need to do is send this information to the shader, with the view direction. The shader will return which fraction for that given light source direction is reflected towards the eye and then multiply the result of the shader with the amount of light produced by this light source. Summing up these results for all possible light sources in the scene gives the total amount of light reflected from P toward the eye (which is the result we are looking for).</p>
<p>Not that in the sum (line 18), there is a third term (a dot product between the normal and the light direction). This term is very important in shading and relates to what we call the <strong>cosine law</strong>. It will be explained in detail in the sections on Light Transport and Shading (you can also find information about it in the lesson on the Rendering Equation which you will find in this section). For now, you should just know that it is there to account for the way light energy is spread across the surface of an object, as the angle between the surface and the light source varies.</p>
<h3 id="conclusion">Conclusion</h3>
<p>There is a fine line between light transport and shading. As we will learn in the section on Light Transport, light transport algorithms will often rely on shaders, to find out in which direction they should spawn secondary rays to compute indirect lighting.</p>
<p>The two things you should remember from this chapter are, the definition of shading and what a shader is:</p>
<ul>
<li>
<p>Shading, is the part of the rendering process that is responsible for computing the amount of light reflected in any given viewing direction. In another word, it is where and when we give objects in the image their final appearance from a particular viewpoint, how they look, their color, their texture, their brightness, etc. <strong>Simulating the appearance of an object requires answering one question only: how much light does an object reflect (and in which directions), over the total amount it receives?</strong></p>
</li>
<li>
<p>Shaders are designed to answer this question. You can see a shader as some sort of black box to which you ask the question: &ldquo;if this object is made of wood, if this wood has this given color and this given roughness, if some quantity of light impinges upon this object from the direction (\omega_i), how much of that light would be reflected by this object back in the environment in the direction (\omega_v)?&rdquo;. The shader will answer this question. We like to describe it as a black box, not because what&rsquo;s happening inside that box is mysterious, but more because it can be seen as a separate entity in the rendering system (it serves only one function which is to answer the above question, and answering this question doesn&rsquo;t require the shader to have any over knowledge about the system than the surface property - its roughness, its color, etc. - and the incoming and outgoing direction being considered) which is why shaders in realtime APIs for example (such as OpenGL - but this often true of all rendering systems whether realtime or off-line) are written separately from the rest of your application.</p>
<p>What&rsquo;s happening in this box is not mysterious at all. What gives objects their unique appearance is the result of complex interactions between light particles (photons) and atoms objects are made of. Simulating these interactions is not practical. We observed though, that the result of these interactions is predictable and consistent, and we know that mathematics can be used to &ldquo;model&rdquo;, or represent, how the real world works. A mathematical model is never the same as the real thing, however, it is a convenient way of expressing a complex problem in a compact form, and can be used to compute the solution (or approximation) of a complex problem in a fraction of the time it would take to simulate the real thing. The science of shading is about developing such models to describe the appearance of objects, as a result of the way light interacts with them at the micro- and atomic scale. The complexity of these models depends on the type of surface we want to replicate the appearance of. Models to replicate the appearance of a perfectly diffuse and mirror-like surface are simple. Coming up with good models to replicate the look of glossy and translucent surfaces is a much harder task.</p>
<p>These models will be studied in the lessons from this section devoted to shading.</p>
</li>
</ul>
<blockquote>
<details>
In the past, techniques used to render 3D scenes in real-time were very much predefined by the API with little control given to the users to change them. Realtime technologies moved away from that paradigm to offer a more programmable pipeline in which each step of the rendering process is controlled by separate "programs" called "shaders". The current OpenGL APIs now support four of such "shaders": the vertex, the geometry, the tessellation, and the fragment shader. The shader in which the color of a point in the image is computed is the fragment shader. The other shaders have little to do with defining the object's look. You should be aware that the term "shader" is therefore generally used now in a broader sense.
</details>
</blockquote>
<h2 id="summary-and-other-considerations-about-rendering">Summary and Other Considerations About Rendering</h2>
<h3 id="summary-4">Summary</h3>
<p>We are not going to repeat what we explained already in the last chapters. Let&rsquo;s just make a list of the terms or concepts you should remember from this lesson:</p>
<ul>
<li>Computers deal with discrete structures which is an issue, as the shapes, we want to represent in images are continuous.</li>
<li>The triangle is a good choice of rendering primitive regardless of the method you use to solve the visibility problem (ray tracing or rasterization).</li>
<li>Rasterization is faster than ray tracing to solve the visibility process (and is the method used by GPUs), but it is easier to simulate global illumination effects with ray tracing. Plus, ray tracing can be used to both solve the visibility problem and shading. If you use rasterization, you need another algorithm or method to compute global illumination (but it is not impossible).</li>
<li>Ray tracing has its issues and challenges though. The ray-geometry intersection test is expensive and the render time increases linearly with the amount of geometry in the scene. Acceleration structures can be used to cut the render time down, but a good acceleration structure is hard to find (one that works well for all possible scene configurations). Ray tracing introduces noise in the image, a visual artifact that is hard to get rid of, etc.</li>
<li>If you decide to use ray tracing to compute shading and simulate global illumination effects, then you will need to simulate the different paths light rays take to get from light sources to the eye. This path depends on the type of surface the ray will interact with on its way to the eye: is the surface diffuse, specular, transparent, etc? There are different ways you can simulate these light paths. Simulating them accurately is important as they make it possible to reproduce lighting effects such as diffuse and specular inter-reflections, caustics, soft shadows, translucency, etc. A good light transport algorithm simulates all possible light paths efficiently.</li>
<li>While it&rsquo;s possible to simulate the transport of light rays from surface to surface, it&rsquo;s impossible to simulate the interaction of light with matter at the micro- and atomic scale. However, the result of these interactions is predictable and consistent. Thus we can attempt at simulating them using a mathematical function. A shader implements some mathematical model to approximate the way a given surface reflects light. The way a surface reflects light is the visual signature of that object. This is how and why we are capable of visually identifying what an object is made of: skin, wood, metal, fabric, plastic, etc., therefore, being able to simulate the appearance of any given material is of critical importance in the process of generating photo-realistic computer-generated images. Again this is the job of shaders.</li>
<li>There is a fine line between shaders and light transport algorithms. How secondary rays are spawned from the surface to compute indirect lighting effects (such as indirect specular and diffuse reflections) depends on the object material type: is the object diffuse, specular, etc? We will learn in the section on light transport, how shaders are used to generate these secondary rays.</li>
</ul>
<blockquote>
<details>
One of the things that we haven't talked about in the previous chapters is the difference between rendering on the CPU vs rendering on the GPU. Don't associate the term GPU with real-time rendering and the term CPU with offline rendering. Real-time and offline rendering have both very precise meanings and have nothing to do with the CPU or the GPU. We speak of **real-time** rendering when a scene can be rendered from 24 to 120 frames per second (24 to 30 fps is the minimum required to give the illusion of movement. A video game typically runs around 60 fps). Anything below 24 fps and above 1 frame per second is considered to be **interactive rendering**. When a frame takes from a few seconds to a few minutes or hours to render, we are then in the category of **offline rendering**. It is very well possible to achieve interactive or even real-time frame rates on the CPU. How much time it takes to render a frame depends essentially on the scene complexity anyway. A very complex scene can take more than a few seconds to render on the GPU. Our point here is that you should not associate GPU with real-time and CPU with offline rendering. These are different things. In the lessons of this section, we will learn how to use OpenGL to render images on the GPU, and we will implement the rasterization and the ray-tracing algorithm on the CPU. We will write a lesson dedicated to looking at the pros and cons of rendering on the GPU or the CPU.
</details>
</blockquote>
<blockquote>
<details>
The other thing we won't be talking about in this section is how rendering and **signal processing** relate to each other. This is a very important aspect of rendering, however, to understand this relationship you need to have solid foundations in signal processing which potentially also requires an understanding of Fourier analysis. We are planning to write a series of lessons on these topics once the basic section is complete. We think it's better to ignore this aspect of rendering if you don't have a good understanding of the theory behind it, rather than presenting it without being able to explain why and how it works.
</details>
</blockquote>
<p><a href="#R-image-d740814b3c9f4a030bf570b269085a41" class="lightbox-link"><img src="../assets/lighteffect.png" alt="" class="figure-image bg-white border lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-d740814b3c9f4a030bf570b269085a41"><img src="../assets/lighteffect.png" alt="" class="lightbox-image bg-white border lightbox noshadow" loading="lazy"></a></p>
<p><strong>Figure 1:</strong> <em>we will also need to learn how to simulate depth of field (top) and motion blur (bottom).</em></p>
<p>Now that we have reviewed these concepts you know what you can expect to find in the different sections devoted to rendering, especially the sections on light transport, ray tracing, and shading. In the section on light transport, we will of course speak about the different ways global illumination effects can be simulated. In the section devoted to ray-tracing techniques, we will study techniques specific to ray tracing such as acceleration structures, ray differentials (don&rsquo;t worry if you don&rsquo;t know what the is for now), etc. In the section on shading, we will learn about what shaders are, we will study the most popular mathematical models developed to simulate the appearance of various materials.</p>
<p>We also talk about purely engineering topics such as multi-threading, multi-processing, or simply different ways the hardware can be used to accelerate rendering.</p>
<p>Finally and more importantly, if you are new to rendering and before you start reading any lessons from these advanced sections, we recommend that you read the next lessons from this section. You will learn about the most basic and important techniques used in rendering:</p>
<ul>
<li>How do the perspective and orthographic projections work? We will learn how to project points onto the surface of a &ldquo;virtual canvas&rdquo; using the perspective projection matrix to create images of 3D objects.</li>
<li>How does ray tracing work? How do we generate rays from the camera to generate an image?</li>
<li>How do we compute the intersection of a ray with a triangle?</li>
<li>How do we render more complex shapes than a simple triangle?</li>
<li>How do we render other basic shapes, such as spheres, disks, planes, etc?</li>
<li>How do we simulate things such as the motion blur of objects, or optical effects such as depth of field?</li>
<li>We will also learn more about the rasterization algorithm and learn how to implement the famous REYES algorithm.</li>
<li>We will also learn about shaders, we will learn about Monte-Carlo ray tracing, and finally texturing. Texturing is a technique used to add surface details to an object. A texture can be an image but also be generated procedurally.</li>
</ul>
<p>Ready?</p>

            <footer class="footline">
            </footer>
          </article>
        </div>
      </main>
    </div>
    <aside id="R-sidebar" class="default-animation">
      <div id="R-header-topbar" class="default-animation"></div>
      <div id="R-header-wrapper" class="default-animation">
        <div id="R-header" class="default-animation">
          
<span style="font-size:26px">晟世清风</span>
        </div>
        <form action="../../../../../search.html" method="get"><div class="searchbox default-animation">
          <button class="search-detail" type="submit" title="Search (CTRL+ALT+f)"><i class="fas fa-search"></i></button>
          <label class="a11y-only" for="R-search-by">Search</label>
          <input data-search-input id="R-search-by" name="search-by" class="search-by" type="search" placeholder="Search...">
          <button class="search-clear" type="button" data-search-clear="" title="Clear search"><i class="fas fa-times" title="Clear search"></i></button>
        </div></form>
        <script>
          var contentLangs=['en'];
        </script>
        <script src="../../../../../js/auto-complete.js?1708235001" defer></script>
        <script src="../../../../../js/lunr/lunr.min.js?1708235001" defer></script>
        <script src="../../../../../js/lunr/lunr.stemmer.support.min.js?1708235001" defer></script>
        <script src="../../../../../js/lunr/lunr.multi.min.js?1708235001" defer></script>
        <script src="../../../../../js/lunr/lunr.en.min.js?1708235001" defer></script>
        <script src="../../../../../js/search.js?1708235001" defer></script>
      </div>
      <div id="R-homelinks" class="default-animation homelinks">
        <ul>
          <li><a class="padding" href="../../../../../index.html"><i class='fas fa-home'></i> Home</a></li>
        </ul>
        <hr class="padding">
      </div>
      <div id="R-content-wrapper" class="highlightable">
        <div id="R-topics">
          <ul class="enlarge morespace collapsible-menu">
          <li data-nav-id="/programming/index.html" class=""><input type="checkbox" id="R-section-d205f2b94e06f539e77f2bc392064cb6" aria-controls="R-subsections-d205f2b94e06f539e77f2bc392064cb6"><label for="R-section-d205f2b94e06f539e77f2bc392064cb6"><i class="fas fa-chevron-down"></i><i class="fas fa-chevron-right"></i><span class="a11y-only">Submenu 程序设计与开发</span></label><a class="padding" href="../../../../../programming/index.html">程序设计与开发</a><ul id="R-subsections-d205f2b94e06f539e77f2bc392064cb6" class="morespace collapsible-menu">
          <li data-nav-id="/programming/java/index.html" class="alwaysopen"><input type="checkbox" id="R-section-1a585580994f9dab2d4547c8eaf11987" aria-controls="R-subsections-1a585580994f9dab2d4547c8eaf11987" checked><label for="R-section-1a585580994f9dab2d4547c8eaf11987"><i class="fas fa-chevron-down"></i><i class="fas fa-chevron-right"></i><span class="a11y-only">Submenu Java 语言</span></label><a class="padding" href="../../../../../programming/java/index.html">Java 语言</a><ul id="R-subsections-1a585580994f9dab2d4547c8eaf11987" class="morespace collapsible-menu">
          <li data-nav-id="/programming/java/java_basics/index.html" class=""><a class="padding" href="../../../../../programming/java/java_basics/index.html">Java 基础知识</a></li>
          <li data-nav-id="/programming/java/java_collection_framework/index.html" class=""><a class="padding" href="../../../../../programming/java/java_collection_framework/index.html">Java 集合框架</a></li>
          <li data-nav-id="/programming/java/java_concurrency/index.html" class=""><a class="padding" href="../../../../../programming/java/java_concurrency/index.html">Java 并发编程</a></li>
          <li data-nav-id="/programming/java/java_memory_model/index.html" class=""><a class="padding" href="../../../../../programming/java/java_memory_model/index.html">Java 内存模型</a></li>
          <li data-nav-id="/programming/java/java_annotation/index.html" class=""><a class="padding" href="../../../../../programming/java/java_annotation/index.html">Java 注解相关</a></li></ul></li></ul></li>
          <li data-nav-id="/test/index.html" class=""><input type="checkbox" id="R-section-40aaf63da4d8c6f43df4cc726b56321f" aria-controls="R-subsections-40aaf63da4d8c6f43df4cc726b56321f"><label for="R-section-40aaf63da4d8c6f43df4cc726b56321f"><i class="fas fa-chevron-down"></i><i class="fas fa-chevron-right"></i><span class="a11y-only">Submenu Test</span></label><a class="padding" href="../../../../../test/index.html">Test</a><ul id="R-subsections-40aaf63da4d8c6f43df4cc726b56321f" class="morespace collapsible-menu">
          <li data-nav-id="/test/demo/index.html" class=""><a class="padding" href="../../../../../test/demo/index.html">Test</a></li></ul></li>
          <li data-nav-id="/algorithms_and_data_structures/index.html" class=""><a class="padding" href="../../../../../algorithms_and_data_structures/index.html">数据结构与算法</a></li>
          <li data-nav-id="/computer_graphics/index.html" class="parent "><input type="checkbox" id="R-section-17dabdd0438dc68716ce11b90ee5aa30" aria-controls="R-subsections-17dabdd0438dc68716ce11b90ee5aa30" checked><label for="R-section-17dabdd0438dc68716ce11b90ee5aa30"><i class="fas fa-chevron-down"></i><i class="fas fa-chevron-right"></i><span class="a11y-only">Submenu 计算机图形学</span></label><a class="padding" href="../../../../../computer_graphics/index.html">计算机图形学</a><ul id="R-subsections-17dabdd0438dc68716ce11b90ee5aa30" class="morespace collapsible-menu">
          <li data-nav-id="/computer_graphics/classic_tutorial/index.html" class="parent "><input type="checkbox" id="R-section-0e0951862a264269c87df7fac4f297fb" aria-controls="R-subsections-0e0951862a264269c87df7fac4f297fb" checked><label for="R-section-0e0951862a264269c87df7fac4f297fb"><i class="fas fa-chevron-down"></i><i class="fas fa-chevron-right"></i><span class="a11y-only">Submenu 经典教程</span></label><a class="padding" href="../../../../../computer_graphics/classic_tutorial/index.html">经典教程</a><ul id="R-subsections-0e0951862a264269c87df7fac4f297fb" class="morespace collapsible-menu">
          <li data-nav-id="/computer_graphics/classic_tutorial/basics_tutorial/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/basics_tutorial/index.html">基础教程</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/shader_tutorial/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/shader_tutorial/index.html">着色教程</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/raytracing_tutorial/index.html" class="parent "><input type="checkbox" id="R-section-06ac295778bd74c824886a3ab22c212c" aria-controls="R-subsections-06ac295778bd74c824886a3ab22c212c" checked><label for="R-section-06ac295778bd74c824886a3ab22c212c"><i class="fas fa-chevron-down"></i><i class="fas fa-chevron-right"></i><span class="a11y-only">Submenu 光线追踪</span></label><a class="padding" href="../../../../../computer_graphics/classic_tutorial/raytracing_tutorial/index.html">光线追踪</a><ul id="R-subsections-06ac295778bd74c824886a3ab22c212c" class="morespace collapsible-menu">
          <li data-nav-id="/computer_graphics/classic_tutorial/raytracing_tutorial/ray_tracing_in_one_weekend/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/raytracing_tutorial/ray_tracing_in_one_weekend/index.html">Ray Tracing in One Weekend</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/raytracing_tutorial/ray_tracing_the_next_week/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/raytracing_tutorial/ray_tracing_the_next_week/index.html">Ray Tracing The Next Week</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/raytracing_tutorial/ray_tracing_the_rest_of_your_life/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/raytracing_tutorial/ray_tracing_the_rest_of_your_life/index.html">Ray Tracing The Rest of Your Life</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/raytracing_tutorial/scratchapixel/index.html" class="parent "><input type="checkbox" id="R-section-c33da7e0886830ccf91b260ac3fff456" aria-controls="R-subsections-c33da7e0886830ccf91b260ac3fff456" checked><label for="R-section-c33da7e0886830ccf91b260ac3fff456"><i class="fas fa-chevron-down"></i><i class="fas fa-chevron-right"></i><span class="a11y-only">Submenu Scratchapixe 系列短文</span></label><a class="padding" href="../../../../../computer_graphics/classic_tutorial/raytracing_tutorial/scratchapixel/index.html">Scratchapixe 系列短文</a><ul id="R-subsections-c33da7e0886830ccf91b260ac3fff456" class="morespace collapsible-menu">
          <li data-nav-id="/computer_graphics/classic_tutorial/raytracing_tutorial/scratchapixel/scratchapixel-chapter-1/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/raytracing_tutorial/scratchapixel/scratchapixel-chapter-1/index.html">Chapter - 1</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/raytracing_tutorial/scratchapixel/scratchapixel-chapter-2/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/raytracing_tutorial/scratchapixel/scratchapixel-chapter-2/index.html">Chapter - 2</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/raytracing_tutorial/scratchapixel/scratchapixel-chapter-3/index.html" class="active"><a class="padding" href="../../../../../computer_graphics/classic_tutorial/raytracing_tutorial/scratchapixel/scratchapixel-chapter-3/index.html">Chapter - 3</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/raytracing_tutorial/scratchapixel/scratchapixel-chapter-4/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/raytracing_tutorial/scratchapixel/scratchapixel-chapter-4/index.html">Chapter - 4</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/raytracing_tutorial/scratchapixel/scratchapixel-chapter-5/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/raytracing_tutorial/scratchapixel/scratchapixel-chapter-5/index.html">Chapter - 5</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/raytracing_tutorial/scratchapixel/scratchapixel-chapter-6/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/raytracing_tutorial/scratchapixel/scratchapixel-chapter-6/index.html">Chapter - 6</a></li></ul></li></ul></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/index.html" class=""><input type="checkbox" id="R-section-23d956a7f1e516a9e3d46517dfb799dd" aria-controls="R-subsections-23d956a7f1e516a9e3d46517dfb799dd"><label for="R-section-23d956a7f1e516a9e3d46517dfb799dd"><i class="fas fa-chevron-down"></i><i class="fas fa-chevron-right"></i><span class="a11y-only">Submenu OpenGL教程</span></label><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/index.html">OpenGL教程</a><ul id="R-subsections-23d956a7f1e516a9e3d46517dfb799dd" class="morespace collapsible-menu">
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/learn_opengl/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/learn_opengl/index.html">Learn OpenGL</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/index.html" class=""><input type="checkbox" id="R-section-ae6b71edd18a266dbd92162060f7614e" aria-controls="R-subsections-ae6b71edd18a266dbd92162060f7614e"><label for="R-section-ae6b71edd18a266dbd92162060f7614e"><i class="fas fa-chevron-down"></i><i class="fas fa-chevron-right"></i><span class="a11y-only">Submenu OGL dev 教程</span></label><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/index.html">OGL dev 教程</a><ul id="R-subsections-ae6b71edd18a266dbd92162060f7614e" class="morespace collapsible-menu">
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_1_open_a_window/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_1_open_a_window/index.html">Chapter - 1</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_2_hello_dot/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_2_hello_dot/index.html">Chapter - 2</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_3_first_triangle/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_3_first_triangle/index.html">Chapter - 3</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_4_shaders/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_4_shaders/index.html">Chapter - 4</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_5_uniform_variables/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_5_uniform_variables/index.html">Chapter - 5</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_6_translation_transformation/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_6_translation_transformation/index.html">Chapter - 6</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_7_rotation_transformation/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_7_rotation_transformation/index.html">Chapter - 7</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_8_scaling_transformation/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_8_scaling_transformation/index.html">Chapter - 8</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_9_interpolation/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_9_interpolation/index.html">Chapter - 9</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_10_indexed_draws/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_10_indexed_draws/index.html">Chapter - 10</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_11_concatenating_transformations/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_11_concatenating_transformations/index.html">Chapter - 11</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_12_perspective_projection/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_12_perspective_projection/index.html">Chapter - 12</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_13_camera_space/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_13_camera_space/index.html">Chapter - 13</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_14_camera_control_-_part_1/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_14_camera_control_-_part_1/index.html">Chapter - 14</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_15_camera_control_-_part_2/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_15_camera_control_-_part_2/index.html">Chapter - 15</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_16_basic_texture_mapping/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_16_basic_texture_mapping/index.html">Chapter - 16</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_17_ambient_lighting/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_17_ambient_lighting/index.html">Chapter - 17</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_18_diffuse_lighting/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_18_diffuse_lighting/index.html">Chapter - 18</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_19_specular_lighting/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_19_specular_lighting/index.html">Chapter - 19</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_20_point_light/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_20_point_light/index.html">Chapter - 20</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_21_spot_light/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_21_spot_light/index.html">Chapter - 21</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_22_loading_models_using_the_open_asset_import_library/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_22_loading_models_using_the_open_asset_import_library/index.html">Chapter - 22</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_23_shadow_mapping_-_part_1/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_23_shadow_mapping_-_part_1/index.html">Chapter - 23</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_24_shadow_mapping_-_part_2/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_24_shadow_mapping_-_part_2/index.html">Chapter - 24</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_25_skybox/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_25_skybox/index.html">Chapter - 25</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_26_normal_mapping/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_26_normal_mapping/index.html">Chapter - 26</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_27_billboarding_and_the_geometry_shader/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_27_billboarding_and_the_geometry_shader/index.html">Chapter - 27</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_28_particle_system_using_transform_feedback/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_28_particle_system_using_transform_feedback/index.html">Chapter - 28</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_29_3d_picking/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_29_3d_picking/index.html">Chapter - 29</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_30_basic_tessellation/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_30_basic_tessellation/index.html">Chapter - 30</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_31_pn_triangles_tessellation/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_31_pn_triangles_tessellation/index.html">Chapter - 31</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_32_vertex_array_objects/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_32_vertex_array_objects/index.html">Chapter - 32</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_33_instanced_rendering/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_33_instanced_rendering/index.html">Chapter - 33</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_34_glfx_-_an_opengl_effect_library/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_34_glfx_-_an_opengl_effect_library/index.html">Chapter - 34</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_35_deferred_shading_-_part_1/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_35_deferred_shading_-_part_1/index.html">Chapter - 35</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_36_deferred_shading_-_part_2/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_36_deferred_shading_-_part_2/index.html">Chapter - 36</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_37_deferred_shading_-_part_3/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_37_deferred_shading_-_part_3/index.html">Chapter - 37</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_38_skeletal_animation_with_assimp/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_38_skeletal_animation_with_assimp/index.html">Chapter - 38</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_39_silhouette_detection/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_39_silhouette_detection/index.html">Chapter - 39</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_40_stencil_shadow_volume/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_40_stencil_shadow_volume/index.html">Chapter - 40</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_41_object_motion_blur/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_41_object_motion_blur/index.html">Chapter - 41</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_42_percentage_closer_filtering/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_42_percentage_closer_filtering/index.html">Chapter - 42</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_43_multipass_shadow_mapping_with_point_lights/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_43_multipass_shadow_mapping_with_point_lights/index.html">Chapter - 43</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_44_glfw/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_44_glfw/index.html">Chapter - 44</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_45_screen_space_ambient_occlusion/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_45_screen_space_ambient_occlusion/index.html">Chapter - 45</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_46_ssao_with_depth_reconstruction/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_46_ssao_with_depth_reconstruction/index.html">Chapter - 46</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_47_shadow_mapping_with_directional_lights/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_47_shadow_mapping_with_directional_lights/index.html">Chapter - 47</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_48_user_interface_with_ant_tweak_bar/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_48_user_interface_with_ant_tweak_bar/index.html">Chapter - 48</a></li>
          <li data-nav-id="/computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_49_cascaded_shadow_mapping/index.html" class=""><a class="padding" href="../../../../../computer_graphics/classic_tutorial/opengl_tutorial/ogl_dev_modern_opengl_tutorial/tutorial_49_cascaded_shadow_mapping/index.html">Chapter - 49</a></li></ul></li></ul></li></ul></li>
          <li data-nav-id="/computer_graphics/book_recommend/index.html" class=""><a class="padding" href="../../../../../computer_graphics/book_recommend/index.html">书籍推荐</a></li>
          <li data-nav-id="/computer_graphics/blog_article/index.html" class=""><a class="padding" href="../../../../../computer_graphics/blog_article/index.html">博客文章</a></li>
          <li data-nav-id="/computer_graphics/reference/index.html" class=""><a class="padding" href="../../../../../computer_graphics/reference/index.html">参考资料</a></li></ul></li>
          <li data-nav-id="/computer_science/index.html" class=""><input type="checkbox" id="R-section-7d71eaebfb32275fa70e6b3397a5fe7c" aria-controls="R-subsections-7d71eaebfb32275fa70e6b3397a5fe7c"><label for="R-section-7d71eaebfb32275fa70e6b3397a5fe7c"><i class="fas fa-chevron-down"></i><i class="fas fa-chevron-right"></i><span class="a11y-only">Submenu 计算机科学</span></label><a class="padding" href="../../../../../computer_science/index.html">计算机科学</a><ul id="R-subsections-7d71eaebfb32275fa70e6b3397a5fe7c" class="morespace collapsible-menu">
          <li data-nav-id="/computer_science/blog_article/index.html" class=""><a class="padding" href="../../../../../computer_science/blog_article/index.html">博客文章</a></li>
          <li data-nav-id="/computer_science/operating_system/index.html" class=""><a class="padding" href="../../../../../computer_science/operating_system/index.html">操作系统</a></li></ul></li>
          <li data-nav-id="/computer_networks/index.html" class=""><a class="padding" href="../../../../../computer_networks/index.html">计算机网络</a></li>
          <li data-nav-id="/game_programming/index.html" class=""><a class="padding" href="../../../../../game_programming/index.html">游戏编程</a></li>
          <li data-nav-id="/database/index.html" class=""><a class="padding" href="../../../../../database/index.html">数据库</a></li>
          </ul>
        </div>
        <div id="R-shortcuts">
          <div class="nav-title padding">More</div>
          <ul class="space">
            <li><a class="padding" href="https://github.com/McShelby/hugo-theme-relearn"><i class='fab fa-fw fa-github'></i> GitHub repo</a></li>
            <li><a class="padding" href="../../../../../tags/index.html"><i class='fas fa-fw fa-tags'></i> Tags</a></li>
            <li><a class="padding" href="../../../../../categories/index.html"><i class='fas fa-fw fa-layer-group'></i> Categories</a></li>
          </ul>
        </div>
        <div class="padding footermargin footerLangSwitch footerVariantSwitch footerVisitedLinks footerFooter showVariantSwitch showFooter"></div>
        <div id="R-menu-footer">
          <hr class="padding default-animation footerLangSwitch footerVariantSwitch footerVisitedLinks footerFooter showVariantSwitch showFooter">
          <div id="R-prefooter" class="footerLangSwitch footerVariantSwitch footerVisitedLinks showVariantSwitch">
            <ul>
              <li id="R-select-language-container" class="footerLangSwitch">
                <div class="padding menu-control">
                  <i class="fas fa-language fa-fw"></i>
                  <span>&nbsp;</span>
                  <div class="control-style">
                    <label class="a11y-only" for="R-select-language">Language</label>
                    <select id="R-select-language" onchange="location = baseUri + this.value;">
                    </select>
                  </div>
                  <div class="clear"></div>
                </div>
              </li>
              <li id="R-select-variant-container" class="footerVariantSwitch showVariantSwitch">
                <div class="padding menu-control">
                  <i class="fas fa-paint-brush fa-fw"></i>
                  <span>&nbsp;</span>
                  <div class="control-style">
                    <label class="a11y-only" for="R-select-variant">Theme</label>
                    <select id="R-select-variant" onchange="window.variants && variants.changeVariant( this.value );">
                      <option id="R-auto" value="auto" selected>Auto</option>
                      <option id="R-relearn-light" value="relearn-light">Relearn Light</option>
                      <option id="R-relearn-dark" value="relearn-dark">Relearn Dark</option>
                      <option id="R-zen-light" value="zen-light">Zen Light</option>
                      <option id="R-zen-dark" value="zen-dark">Zen Dark</option>
                      <option id="R-neon" value="neon">Neon</option>
                      <option id="R-learn" value="learn">Learn</option>
                      <option id="R-blue" value="blue">Blue</option>
                      <option id="R-green" value="green">Green</option>
                      <option id="R-red" value="red">Red</option>
                    </select>
                  </div>
                  <div class="clear"></div>
                </div>
                <script>window.variants && variants.markSelectedVariant();</script>
              </li>
              <li class="footerVisitedLinks">
                <div class="padding menu-control">
                  <i class="fas fa-history fa-fw"></i>
                  <span>&nbsp;</span>
                  <div class="control-style">
                    <button onclick="clearHistory();">Clear History</button>
                  </div>
                  <div class="clear"></div>
                </div>
              </li>
            </ul>
          </div>
          <div id="R-footer" class="footerFooter showFooter">
          <span class="github-buttons"></span>
          <p>Built with <a href="https://github.com/McShelby/hugo-theme-relearn" title="love"><i class="fas fa-heart"></i></a> by <a href="https://gohugo.io/">Hugo</a></p>
          <script>
            function githubButtonsScheme(){
              var scheme = 'light';
              var colorPropertyValue = window.getComputedStyle( document.querySelector( '#R-sidebar' ) ).getPropertyValue( 'background-color' );
              var colorValues = colorPropertyValue.match( /\d+/g ).map( function( e ){ return parseInt(e,10); });
              if( colorValues.length === 3 && ((0.2126 * colorValues[0]) + (0.7152 * colorValues[1]) + (0.0722 * colorValues[2]) < 165) ){
                
                scheme = 'dark';
              }
              return scheme;
            }
            function githubButtonsInit(){
              if( !window.githubButtons ){
                
                setTimeout( githubButtonsInit, 50 );
                return;
              }
              var scheme = githubButtonsScheme();
              var githubButtonsHTML = `
                <a class="github-button" href="https://github.com/McShelby/hugo-theme-relearn/archive/main.zip" data-color-scheme="${scheme}" data-icon="octicon-cloud-download" aria-label="Download McShelby/hugo-theme-relearn on GitHub">Download</a>
                <a class="github-button" href="https://github.com/McShelby/hugo-theme-relearn" data-color-scheme="${scheme}" data-icon="octicon-star" data-show-count="true" aria-label="Star McShelby/hugo-theme-relearn on GitHub">Star</a>
                <a class="github-button" href="https://github.com/McShelby/hugo-theme-relearn/fork" data-color-scheme="${scheme}" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork McShelby/hugo-theme-relearn on GitHub">Fork</a>
               `;
              document.querySelector( '.github-buttons' ).innerHTML = githubButtonsHTML;
              document.querySelectorAll( '.github-button' ).forEach( function( anchor ){
                anchor.dataset.colorScheme = scheme;
                window.githubButtons.render( anchor, function( el ){
                  anchor.parentNode.replaceChild( el, anchor );
                });
              });
            }
            document.addEventListener( 'themeVariantLoaded', function( e ){
              
              setTimeout( githubButtonsInit, 400 );
            });
          </script>
          <script async src="../../../../../js/github-buttons.js?1708235001"></script>
          </div>
        </div>
      </div>
    </aside>
    <script src="../../../../../js/clipboard.min.js?1708235001" defer></script>
    <script src="../../../../../js/perfect-scrollbar.min.js?1708235001" defer></script>
    <script>
      function useMathJax( config ){
        if( !Object.assign ){
          
          return;
        }
        window.MathJax = Object.assign( window.MathJax || {}, {
          loader: {
            load: ['[tex]/mhchem']
          },
          startup: {
            elements: [
              '.math'
            ]
          },
          tex: {
            inlineMath: [
              ['$', '$'], 
              ['\\(', '\\)']
            ]
          },
          options: {
            enableMenu: false 
          }
        }, config );
      }
      useMathJax( JSON.parse("{}") );
    </script>
    <script id="MathJax-script" async src="../../../../../js/mathjax/tex-mml-chtml.js?1708235001"></script>
    <script src="../../../../../js/theme.js?1708235001" defer></script>
  </body>
</html>
